# NLP的发展历程

##  一，基于统计学方法的阶段

### 1，分布式假设的提出

单词本身没有含义，单词的含义由它所在的上下文（语境）形成

### 2，共现矩阵和词向量

eg:  **you say goodbye and i say hello.**

（1）对周围单词的数量进行计数得到**词向量**

|      | you  | say  | goodbye | and  |  i   | hello |  .   |
| :--: | :--: | :--: | :-----: | :--: | :--: | :---: | :--: |
| you  |  0   |  1   |    0    |  0   |  0   |   0   |  0   |

（2）汇总所有单词得到**共现矩阵**

|         | you  | say  | goodbye | and  |  i   | hello |  .   |
| :-----: | :--: | :--: | :-----: | :--: | :--: | :---: | :--: |
|   you   |  0   |  1   |    0    |  0   |  0   |   0   |  0   |
|   say   |  1   |  0   |    1    |  0   |  1   |   1   |  0   |
| goodbye |  0   |  1   |    0    |  1   |  0   |   0   |  0   |
|   and   |  0   |  0   |    1    |  0   |  1   |   0   |  0   |
|    i    |  0   |  1   |    0    |  1   |  0   |   0   |  0   |
|  hello  |  0   |  1   |    0    |  0   |  0   |   0   |  1   |
|    .    |  0   |  1   |    0    |  0   |  0   |   1   |  0   |

（3）计算词向量间的相似度（余弦相似度）并排序

（4）使用PMI（点互信息）

它是共现矩阵的一种**增强方式**，用于衡量两个词之间的语义关联强度，避免高频词（如“的”“是”）对结果造成干扰。



**早期的词向量方法通过统计一个词周围词的出现频率，把词变成“向量”，再通过计算“词向量之间的角度”（余弦相似度），来比较它们的意思是否接近。这样做的目的是让计算机可以“理解”词之间的关系，比如“苹果”和“橘子”更像，而不是“苹果”和“汽车”。**



### 3,统计学方法的缺点

（1）维度灾难

统计共现矩阵通常是词表大小的平方，随着语料库词汇量的增加，维度会变得非常大

（2）效率低下

构建和操作这样巨大的矩阵非常耗时，不适合大规模语料和实时应用。

（3）词义表达有限

只依赖共现频率，不能很好地捕捉词与词之间复杂的语义关系，比如“国王 - 男人 + 女人 = 女王”这种类比关系难以表达。





## 二，基于人工智能神经网络的阶段

### 1，Word2vec和RNN阶段

#### （1）Word2vec

Word2Vec 是 Google 在 2013 年提出的一种**预测型获取词向量方法，它不像之前的统计共现矩阵方法直接统计词频，而是用神经网络训练模型，通过**上下文预测目标词**（或者反向预测上下文），把词映射成低维的稠密向量。



**CBOW模型**：通过上下文预测目标词

句子是：`I love natural language processing`

现在我们想猜的是“natural”这个词，那 CBOW 的输入就是它前后两个词：

```
复制编辑
上下文词：["I", "love", "language", "processing"]
```

step1：将句子使用one-hot表示（输入层）

```
"I"         -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  
"love"      -> [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  
...
"processing"-> [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  
```

step2:使用句子的one-hot向量去乘以句子的词嵌入向量（输入层权重Win)

```
# 第一次随机初始化词向量
"I"        -> [0.1, -0.3, 0.8]  
"love"     -> [-0.2, 0.4, 0.7]  
"language" -> [0.5, -0.6, 0.1]  
"processing" -> [-0.1, 0.2, 0.9]  
```

step3:得到中间层，再使用中间层乘以输出层权重（Wout)

step4:再通过Softmax with Loss层得到一个概率分布和损失参数

step5:通过误差反向传播更新来更新权重

输入层的权重便是我们要求的词向量



**skip-gram模型**：通过目标词预测上下文



#### （2）RNN（循环神经网络）

一种用于处理**序列数据**的神经网络，它的主要特点是：**可以“记住”之前的输入信息**，并将其用于当前的预测任务。
 这让 RNN 特别适合处理像文本、语音、时间序列这类**前后相关性强**的任务。

（数据需要按顺序输入）

RNN 的神经元不仅接收当前时刻的输入，还接收 **上一个时刻的隐藏状态**（记忆），并输出当前的隐藏状态。

```
xt: 当前输入
ht: 当前隐藏状态
ht-1: 上一个隐藏状态

ht = tanh(Wx * xt + Wh * ht-1 + b)
```

每个时间步共享权重（Wx, Wh），并通过循环结构处理序列。



缺点：RNN不擅长学习时序数据的长期依赖关系， 如果wh>1（比如等于2） 随着序列长度的增加，会出现**梯度爆炸**的问题

如果wh<1 (比如等于0.5) 随着序列长度的增加，会出现**梯度消失**的问题



#### （3）LSTM（长短期记忆网络）

传统的循环神经网络（RNN）在处理长期依赖关系时容易出现**梯度消失或梯度爆炸**的问题，导致模型难以记住较久远的信息。而**LSTM（长短期记忆网络）**通过引入专门的门控机制（如遗忘门、输入门和输出门），能够有效缓解这些问题，从而更好地捕捉和学习长期依赖关系。



**遗忘门**：**作用**：决定上一时刻的细胞状态 Ct−1C_{t-1}Ct−1 中，哪些信息需要被“遗忘”或保留。
 **计算公式**：

**ft = σ(Wf xt + Uf ht−1 + bf)**

- xt：当前输入向量
- ht−1：上一个时间步的隐藏状态
- Wf, Uf：权重矩阵
- bf：偏置向量
- σ：Sigmoid 函数，输出在 [0,1][0,1][0,1] 之间

当 ft[i]接近 0 时，第 i 维信息被“遗忘”；当接近 1 时，则完全保留。



**输入门**：**作用**：决定当前输入 xtx_txt 中有哪些新信息要写入细胞状态。它实际上由两部分组成：

1. **门控信号** it：控制写入强度
2. **候选信息** Ct：新信息的候选内容

**计算公式**：

**it = σ(Wi xt+Ui ht−1 + bi)**

**Ct = tanh⁡(Wc xt+Uc ht−1 + bc)**

- tanh输出范围在 [][-1,1][−1,1]，用来生成可加到细胞状态的新候选信息。

**细胞状态更新**：

**Ct = ft ⊙ Ct−1  +  it ⊙ Ct**

- ⊙：表示逐元素相乘。
- 第一项保留了上一状态的“被允许”部分，第二项则是写入的新信息。



**输出门**：**作用**：决定从细胞状态 **Ct** 中输出多少信息，并生成当前时刻的隐藏状态 ht。
 **计算公式**：

**ot = σ(Wo xt + Uo ht−1 + bo)**

**ht = ot ⊙ tanh⁡(Ct)**

- 首先用 Sigmoid 生成门控信号 ot，
- 再将细胞状态通过 tanh⁡ 压缩到 [][-1,1][−1,1] 后，乘以 ot，得到最终输出 ht。



**三个门的协同工作流程**

1. **遗忘门** ft决定保留多少旧记忆。
2. **输入门** （it、Ct）决定添加多少新信息。
3. **细胞状态更新**：混合旧记忆与新信息，得到 Ct。
4. **输出门** ot决定从更新后的 Ct中“输出”多少，形成隐藏状态 ht。





### 2,seq2seq模型和Attention（注意力机制）阶段

#### (1)，seq2seq

**Seq2Seq 就是把一个“序列”变成另一个“序列”的神经网络模型。**



seq2seq的构成主要由两个部分：

**Encoder 编码器**

- 它负责“读懂”输入句子。
- 输入一段话，它就一词一词地读进去，把整个句子的意思浓缩成一个向量（叫“上下文向量”context vector）。

 **Decoder 解码器**

- 它负责“生成”输出句子。
- 给它上面那个向量，它就开始一个词一个词地“写出”答案，比如翻译句子或写摘要等。



**训练时的“教师机制”**：训练时Decoder 的输入用“真实词”，预测的是“下一个词”，误差是来自“输出 vs 目标词”的比较，依然可以正常反向传播。



**使用LSTM的seq2seq的缺陷**：Encoder 把整个输入序列的信息，压缩成一个“**固定长度的上下文向量**”（也叫 context vector）传给 Decoder。这就导致当句子太长时，因为压缩的太狠会造成信息丢失。



#### (2)，Attention(注意力机制)

像人类一样将注意力集中在必要的信息上（不再只用编码器的最后一个隐藏状态，而是**把每个时间步的隐藏状态都留下来**，然后在解码的每一步都**“动态地挑重点”**去关注输入序列的不同部分。）



**编码器的改进**：

原来的编码器只保留最后一个隐藏状态，改进之后**保留每一个时间步的隐藏状态**。



**解码器的改进**：

额外增加了一个Attention层，接收编码器中每一个时间步的隐藏状态**hs**，和解码器中LSTM层输出的隐藏状态**h**，输出上下文向量 **c**



**step1**: 求 h 和 hs 的各个向量的相似度（向量内积，指两个向量在多大程度上指向同一方向）**S**

**step2**: 使用softmax函数对s进行正规化得到hs的各个向量的权重分布  **a**

**step3**: 计算单词向量hs和对应的各个单词向量的权重a的加权和，输出上下文向量  **c**



缺陷：LSTM 结构天然就是**一步一步来的**，每个时间步都依赖上一个时间步的输出。

这意味着：

- 无法并行训练
- 推理速度慢，硬件利用率低（比如 GPU）



### 3，Transformer模型和self-Attention(自注意机制)阶段

解决了并行计算的问题，同时具备处理长距离依赖的能力











































































