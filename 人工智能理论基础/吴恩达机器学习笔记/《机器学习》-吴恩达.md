# 《机器学习》-吴恩达

## 第一模块：有监督的机器学习：回归和分类

### 第一周：机器学习入门和单变量回归



#### 2.1，什么是机器学习

定义：对于某类任务T和性能度量p，如果一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么

我们称这个计算机程序从经验E学习。



#### 2.2，Suprvised Learning(监督学习)

（1）定义：监督学习是指在训练模型时使用带有标签的训练数据。这些标签提供了输入数据与目标输出之间的映射关系。

（2）监督学习又分为回归问题和分类问题

回归问题：回归问题涉及预测一个连续的数值输出。即，模型的目标是找到一个函数，能够将输入特征映射到一个连续的数值上。

例如：预测房价

![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-30 200112.png)

分类问题：分类问题涉及将输入数据分配到预定义的类别中。即，模型的目标是将输入特征映射到离散的标签或类别。

例如：判断肿瘤是良性还是恶性

![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-30 200225.png)



#### 2.4，Unsupervised Learning(无监督学习)

（1）定义：无监督学习是指在训练模型时使用没有标签的训练数据。目标是发现数据中的潜在结构或模式，而不是直接进行预测或分类。

（2）聚类：用于将数据分成若干个组或簇，使得同一簇中的数据点具有较高的相似性，而不同簇中的数据点具有较大的差异。

![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-30 200339.png)

（3）异常检测：用于发现数据中的异常模式或不符合常规的样本。

（4）降维：旨在将高维数据映射到低维空间，同时尽可能保留数据的原始结构和特征。



#### 3.1，线性回归模型

（1）认识数据集

![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-28 194950.png)

terminology:专业术语	set:设置	feet:尺寸 	notation:符号 	exponent:指数

训练集：用于训练模型的数据集

m 　　　 代表训练集中实例的数量
x 　　　　代表特征/输入变量
y 　　　　代表目标变量/输出变量
(x,y) 　　 代表训练集中单个实例
(x(i),y(i))  代表第 i 个实例：其中x(i) 代表第i个输入变量, y(i)代表第i个目标变量



（2）线性回归

![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-28 201123.png)

training set:训练数据集（features：特征， targets：目标）

learning algorihm：学习算法

prediction：预测	estimated：估算 		 y是目标值， “y-hat”表示y的预测和估算值



represent：表示

Linear regression：线性回归		one variable：单变量（single feature x：单个特征x）

Univariate：单变量的



#### 3.3，代价（损失）函数

![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-29 083555.png)

parameters：参数		coefficients：系数		weights：权重



w，b做了什么

![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-29 084008.png)

intercept：截距



损失函数

![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-29 084607.png)

损失函数：平方误差损失函数

在机器学习中，不同的人会对不同的问题使用不同的损失函数



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-29 090420.png)

objective：客观的		minimize：最小化



三维

![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-29 090704.png)



等高线图

![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-29 091123.png)



#### 4.1，梯度下降



![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-29 194619.png)

linear regression：线性回归		outline：概述		have：有

从一些w，b开始，可以任意设置

迭代w和b逐渐减小损失函数 J(w,b)

直到我们达到或接近最低限度（可能不止一个）



![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-29 195550.png)

gradient descent：梯度下降		local minima：局部最小值

不是平方误差损失函数也不是线性回归

原理：

你在山坡上的某个位置开始（这就相当于初始化参数值）。

你观察周围的地形，决定哪个方向的坡度最大。这个方向指向山坡下降最快的方向。这个方向就是“梯度”。

你决定沿着这个方向滑下山坡。滑下的距离取决于你对坡度的估计和滑下的速度（相当于“学习率”）。滑得太快可能会错过最低点，滑得太慢则可能需要很长时间才能找到最低点。

你继续滑动，直到到达山坡的最低点或者一个相对平坦的地方。在每一步，你都会重新观察坡度，调整你的方向，直到你找不到比当前点更低的地方为止。



![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-29 200913.png)



gradient descent algorithm：梯度下降算法		repeat until convergence：重复直到收敛

assignment：赋值		truth assertion：真值断言		derivative：派生词		simultaneously：同时

correct：正确的		incorrect：错误的

这里的 “=” 代表的是赋值的含义，而不是数学中的真值断言，在python中使用“==”来表示真值断言

&：学习率		

同时更新w和b



![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-29 204925.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-29 205147.png)

overshoot：超过		converge：收敛		diverge：发散

如果学习率&设置太小，梯度下降会变得很慢

如果学习率&设置太大，1，超过，无法达到最小值 	2，未能收敛，发散



![](C:\Users\23012\OneDrive\Desktop\图片\屏幕截图 2024-08-29 205803.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-29 205956.png)

fixed：固定的

以固定的学习率达到局部最小值

越接近局部最小值，坡度越缓，导数变小，更新的步奏变小

达到局部最小值在没有减少学习率的前提下



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-29 210709.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-29 210915.png)

optional：可选的

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-29 211718.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-29 211843.png)



梯度下降的一个问题，它可能导致局部最小值而不是全局最小值

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-29 212127.png)



但对于线性回归的平方误差成本函数，只有单一的全局最小值

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-29 212410.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-29 212558.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-29 212858.png)

’间歇性‘梯度下降





### 第二周：多输入变量回归

#### 1.1，多维特征

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 202034.png)

multiple:多个		variables：多变量

x上的箭头可加可不加，主要为了强调这是一个向量而不是数字。



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 202446.png)

previously：之前的



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 202710.png)

parameter：参数		vector：向量		dot product：点积

多元线性回归，不是多元回归



#### 1.2，向量化

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 221332.png)

参数和特征

线性代数：从1开始计数

代码：从0开始计数

不使用向量不使用for循环		不使用向量使用for循环		使用向量



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 221826.png)

inparallel：并行		efficient：有效率的		scale：规模		datasets：数据集

对于大规模数据集效率更高



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 222218.png)

梯度下降		parameters：参数		derivative：导数



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 224353.png)

previous：之前的		notation：公式



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 224824.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 224959.png)

梯度下降的代替方案：正态方程

仅用于线性回归，在不迭代的情况下查找w，b

缺点：

不推广到其他学习算法

特征数量大时速度慢

you need know：

正态方程可能用于实现线性回归的机器学习库

梯度下降是查找w，b的推荐方法



#### 2.1，特征缩放

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 225645.png)

特征和系数向量			reasonable：合理的



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 230035.png)

scatterplot：散点图			contourplot：等高线图



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 230449.png)

rescaled：缩放

原来的轮廓图又高又瘦，梯度下降可能会导致来回横条，需要很长一段时间才能找到局部最小值

这种情况下需要缩放特征，让梯度下降更容易找到局部最小值



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 232202.png)

feature scaling：特征缩放



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 232500.png)

mean normalization：均值归一化

average:平均值 u



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 232716.png)

Z-score归一化		standard deviation：标准差

高斯分布



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-08-30 233042.png)

acceptable ranges：可接受的范围

rescale：重新缩放



#### 2.3，判断梯度下降是否收敛

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-04 103906.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-04 104309.png)

correctly：正确的		decrease:减少		iteration：迭代		converged：收敛		varies：变化		curve：曲线		

automatic：自动的		declare：断言		parameters：参数

学习曲线

梯度下降在多少次迭代后收敛在不同的应用程序不一样

自动收敛测试



#### 2.4如何设置学习率

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-04 105713.png)

identify；识别		minus：减		adjust：调整

代码错误或学习率设置太大

对于一个足够小的学习率，J应该在每次迭代后减小

但是学习率设置太小，梯度下降需要更多的迭代次数才能收敛

经验：如果梯度下降不起作用，可以把学习率设置成非常小的数字，看看每次迭代后的成本是否会降低

目的是检查代码错误



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-04 111245.png)

从小的值每次乘以三倍逐渐增加，找到最大合理值，或比最大合理值略小的值



#### 2.5特征工程

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-04 112233.png)

intuition：直觉		design：设计			transforming：变换		combining:组合

使用直觉或已有的知识去设计新的特征，通过已有特征的变换或组合



#### 2.6多项式回归

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-04 112953.png)

非线性函数

特征缩放变得更加重要，需要将特征转换为可比较的值的范围



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-04 113401.png)



### 第三周：分类

#### 1.1动机与目的

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-05 141803.png)

classification：分类		question：问题		email spam：垃圾邮件		transaction fraudulent：交易诈骗

tumor malignant：恶性肿瘤

y只能有两个值		binary classification：二元分类

class，category：分类

absence：缺席的，虚假的		presence：在场的，真实的

 

假设用线性回归

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-05 143014.png)

tumor size：肿瘤大小		threshold malignant：恶性的门槛			diameter：直径

decision boundary：决策边界

worse:misclassified		更糟糕的错误分类

logistic regression：逻辑回归



#### 1.2逻辑回归

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-05 144817.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-05 145058.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-05 145434.png)

interpretation：解释		probability：可能性		given input：给定输入		parameters：范围，参数



#### 1.3决策边界

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-05 151000.png)

threshold：门槛



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-05 151202.png)

decision  boundary决策边界



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-05 151359.png)



#### 2.1逻辑回归中的代价函数

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 135645.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 135852.png)

squared：平方

convex：凸面的

将线性回归的损失函数用到逻辑回归中得到的是非凸函数，不适用

单个训练例示的损失



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 140405.png)

逻辑损失函数

f的取值范围是0到1

当f的预测值越接近y的真实标签时，损失函数最低



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 140946.png)

further：进一步的

当f的预测值越远离f的真实标签，损失函数越高



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 141414.png)

逻辑损失函数的图像是convex的，能达到局部最小值

找到我w，b得到损失函数的最低值j



#### 2.2简化逻辑回归的代价函数

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 141723.png)

简化逻辑回归的代价函数



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 141959.png)

maximum like lihood：数学统计学中的最大似然估计

逻辑回归的损失函数是如何得到的：最大似然估计（不用担心它的细节）

single：单个的		global：全局

单个的全局最小值



#### 3.1实现梯度下降

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 144216.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 144510.png)

同步更新



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 144726.png)

看起来像线性回归但关于x的函数不一样

concept：概念

在线性回归中使用的关于梯度下降的方法在逻辑回归中也适用

监视梯度下降确保它收敛

使用矢量化使梯度下降在逻辑回归中运行的更快

特征缩放



#### 4.1过拟合问题

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 150303.png)

under fit：欠拟合		just right：刚好正确		over fit：过拟合

high bias：高偏差		generalization：泛化		high variance：高方差

欠拟合：对训练集拟合的不好

对训练集拟合的很好

过拟合：对训练集过度拟合



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 151658.png)

classification：分类



#### 4.2解决过拟合

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 151957.png)

方法一：收集更多的训练数据

缺点：训练数据收集难度和部分问题训练数据有限



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-06 152124.png)

方法二：减少特征（选择特征的子集）

insufficient：不足的

缺点：可能会损失有用的特征(信息)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-07 194507.png)

方法三：正则化

parameters ：参数		regularization：正则化		eliminate：消除

缩小参数Wj的大小

对于wj大的值：过拟合

正则化的作用是：让你保留所有特征的同时，防止特征产生过大的影响

正则化只对于wj，是否对b使用正则化影响不大，一般不用



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-07 195618.png)

addressing overfitting：解决过拟合问题



#### 4.3，正则化

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-07 200444.png)

intuition：直觉

如果w3和w4很大，使用此修改过的成本函数，实际上会惩罚模型

因为如果你想最小化这个函数，让这个新的代价函数变小的唯一办法是w3和

w4都很小



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-07 200913.png)

simpler：更简单的

less likely to overfit：不太可能过拟合

如果你有很多特征，你可能不知道最重要的特征以及要惩罚的特征，

通常实现正则化的方法是惩罚所有的特征

“λ”：正则化参数

regularization term：正则化项

b可以包含也可以不包含

这个新的成本函数权衡了你拥有的两个目标:

1,最小化第一项：鼓励算法通过最小化预测值和实际值的平方差来很好的拟合训练数据

2，使参数wj保持较小，这将倾向于减少过度拟合



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-07 203231.png)

目标是最小化参数J（w，b）

高λ值，增加对权重的惩罚（为了降低J，模型在迭代中就会想办法降低参数w的值，过拟合减少，但可能导致模型更简单）

低λ值，减少对权重的惩罚，可能使模型更复杂（过拟合风险增加）。

mean squared error：均方差

regularization term：正则化项

fit data：拟合数据

keep wj small：保持wj较小

λ的值决定了如何在这两个目标之间取得平衡

两个极端：

λ过小：过拟合

λ过大：拟合水平直线或欠拟合



#### 4.4，用于线性回归的正则化方法

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-07 204421.png)

不需要正则化b



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-07 205129.png)

implementing：实现

usual update：通常更新

shrink：收缩

正则化的工作原理：正则化在每次迭代中所做的是将w乘以一个略小于

1的数字，这会稍微缩小wj的值



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-07 205521.png)

derivative：导数

如何得到导数项



#### 4.5，用于逻辑回归的正则化方法

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-07 212306.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-07 212509.png)





## 第二模块：神经网络（深度学习算法）以及决策树

### 第一周：神经网络

#### 1.2，神经元和大脑



对神经网络历史的认识

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-13 135725.png)

Neural networks：神经网络

algorithms：算法     mimic：模仿		resurgence：复兴

起源：试图模仿大脑的算法

在20世纪80年代和90年代初被使用

在20世纪90年代末失宠

2005年左右重新复兴

speech:语音		images：图像		text（NLP）：文本（自然语言处理）



在大脑中的神经元

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-13 140641.png)

neurons：神经元



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-13 140931.png)

biological neuron：生物神经元		simplified mathematical model of a neuron：神经元的简化数学模型

cell body：细胞体		nucleus：细胞核		dendrites：树突		axon：轴突



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-13 141840.png)

为什么神经网络在现在兴起		performance：高性能的		processors：处理器

1，数据量的增加

2，更快的计算机处理器



#### 1.3，需求预测（神经网络）

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-13 143004.png)

demand prediction:需求预测		activation：激活		

probability of being to seller：成为畅销商品的可能性

通过价格预测该商品是否能成为畅销商品

x=price 作为输入

a=f(x) 作为逻辑回归算法的输出

a也被称为激活，它指的是一个神经元向下游其它神经元发送的一个高的输出

逻辑回归算法可以视为大脑中单个神经元非常简化的模型



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-13 150214.png)

input layer：输入层

hidden layer：隐藏层，可以有多个神经元

output layer：输出层

price：价格		shipping cost：运费		marketing：市场营销		material：材料

afford ability：负担能力		awareness：知名度		perceived quality：品质认可度

隐藏层将输入层的4个值输出为3个激活值，输出层将隐藏层的3个输入值输出为一个激活值

1，神经网络在实践中实现特定层中每个神经元的方式：

比如中间这一层，它的每个神经元都可以访问上一层中的每一个值

2，在前面房价预测问题中，我们必需进行手动特征工程已获得更好的特征，但

在神经网络中不需要手动设计特征，虽然在前面我们决定了负担能力，知名度，品质认可度

但神经网络真正好的特性之一是：从数据中训练它时无需明确决定要做什么，神经网络能够自行决定

它想在这个隐藏层中使用的特征是什么

 

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-13 160243.png)

multiple：多个		perception：感知		architecture：架构

multi layer perception：多层感知器（神经网络）

neural network architecture：神经网络架构



#### 1.4，神经网络举例：图像感知

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-14 161324.png)

recognition：识别		pixels：像素		

输入具有1百万像素的图片，输出图片中人的身份



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-14 161846.png)

第一个隐藏层输出一些短边

第二个隐藏层输出一些面部特征

第三个隐藏层输出整个面部

activations are higher level features：激活更高水平的特征



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-14 162349.png)

classification：分类		detected：检测

probability of car detected：检测到汽车的可能性



#### 2.1，神经网络层的工作原理

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-14 163805.png)

notation：符号		numbering：编号

单个神经元—>单层神经网络—>多层神经网络

上标[1]表示激活值的向量来自第一层

下标1,2,3表示该层中的第一个，第二个，第三个神经元



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-14 165340.png)

a scalar value：一个标量值

注意这里的向量a[1]



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-14 165745.png)

category：类别



#### 2.2，更复杂的神经网络

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-14 173243.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-14 173611.png)

l层的激活值，单元（神经元）j

l-1层的输出（先前的层）

l层的参数w和b，单元j

这里的sigmoid函数g也被称为激活函数



#### 2.3，神经网络的前向传播

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-14 174819.png)

handwritten digit recognition：手写数字识别

这里的向量x也可以表示为a[0]



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-14 175124.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-14 175345.png)

propagation：传播

forward propagation：前向传播

这种类型的神经网络架构，最初会有更多的神经元，但随着靠近输出层，隐藏单元的数量会减少



#### 5.1，强人工智能

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-20 133213.png)

ANI：狭义的人工智能

例如：智能扬声器，自动驾驶汽车，网络搜索，农场和工厂里的AI



AGI：广义的人工智能

例如：做任何人类能做的事



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-20 133858.png)

最初设想：模拟足够多的神经元就能模拟人的大脑

行不通的原因

1，逻辑回归模拟的神经元模型实际和任何生物神经元所做的完全不同，它要简单的多

2，直到今天我们任然不知道大脑的工作原理，我们对大脑的认知完全有限



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-20 134508.png)



#### 6.2，矩阵乘法（数学补充）

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-20 135740.png)

dot products：点积		general：一般的		transpose：转置		vector multiplication：向量乘法		equivalent：相等的



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-20 140510.png)

向量矩阵乘法



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-20 140834.png)



#### 6.3，矩阵乘法规则

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-20 141101.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-20 141340.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-20 141456.png)



#### 6.4，矩阵乘法的实现

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-20 141908.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-20 142108.png)

dense：密集的



### 第二周：神经网络训练

#### 2.1，sigmoid激活函数的替代方案

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 151303.png)

demand prediction：需求预测



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 151431.png)

三种最常用的激活函数：

1，线性函数

2，sigmoid函数

3，ReLU函数

后续还会学到softmax函数



#### 2.2，如何选择激活函数

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 152240.png)

为输出层选择激活函数：

二元分类问题：sigmoid函数

可以取负值的回归问题：线性回归函数

只可以取非负值的回归函数（比如房价预测）：ReLU函数



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 152822.png)

为隐藏层选择激活函数：

通常选择ReLU函数：

为什么不选择sigmoid函数？

1，ReLU函数计算速度更快，因为它只需计算0，z

sigmoid函数则需要取幂然后取逆等等

2，ReLU函数只有左边是平的，而sigmoid函数左右两边都是平的，

这导致了梯度下降变得更慢



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 153337.png)

summary：总结



在隐藏层中不推荐使用线性激活函数，因为神经网络仅能进行线性变换，无法处理复杂问题



#### 3.1，多分类问题

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 193557.png)

多分类问题：标签y多于两个以上可能值



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 193759.png)



#### 3.2，softmax激活函数

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 194002.png)

处理有多个可能输出的问题



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 194204.png)

crossentropy loss：交叉熵损失函数



#### 3.3，神经网络的softmax输出

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 194603.png)



#### 3.5，多标签问题

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 194753.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 194903.png)

可以使用三个神经网络来分别输出或者训练一个有三个输出的神经网络

alternative：或者



#### 4.1,高级优化方法

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 200233.png)

更快：增加&				更慢：减少&



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 200609.png)

adam算法：可以自动调整学习率&的算法，使用多个&



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 200717.png)

如果w或b在相同的方向保持前进，就增加学习率&

如果w或b来回摆动，就减小学习率&



#### 4.2，其他的网络层类型

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 205141.png)

dense layer：密集层：每个神经元输出都是上一层所有激活输出的函数

也称为全连接层（Fully Connected Layer, FC）：每个神经元与前一层的所有神经元相连



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 205609.png)

convolutional layer：卷积层（通过卷积操作提取局部特征）

每个神经元只查看上个层的部分输入

为什么？

更快的计算机运行速度

只需要更少的训练集

更不容易过拟合



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-21 210010.png)

EKG:心电图

最后用sigmoid函数判断是否存在心脏病



### 第三周：应用机器学习的建议



#### 1.1，下一步要做什么

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 190153.png)

调试学习算法

你已经对预测房价实现了正则化线性回归

但它在预测中犯了令人无法接受的大错误，你接下来要尝试什么？

获取更多的数据集

尝试更少的特征集

获取更多的特征

尝试添加多项式特征

尝试减少λ

尝试增加λ



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 130001.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 190901.png)

机器学习诊断

诊断：运行一个测试来深入了解学习算法中不工作的内容，以获得提高性能的指导

诊断需要时间去实施，但这样做能够帮助你更好的利用自己的时间



#### 1.2，模型评估1

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 191545.png)

evaluating：评估

评估你的模型

模型非常适合你的训练集，但无法推广到训练集中没有的新示例

我们需要一些更系统化的方法来评估模型的表现



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 191942.png)

将数据集进行划分，70%为训练集，30%为测试集



![屏幕截图 2024-09-23 192355](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 192355.png)

对于线性回归的训练/测试程序（使用平方误差损失函数）

通过最小化成本函数来拟合参数J（w，b）

计算测试误差

计算训练误差



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 192829.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 192946.png)

对于二元分类问题的训练/测试程序



方法：获取一个数据集并将其拆分为一个单独的训练集和一个单独的测试集，

通过计算j测试和j训练，你现在可以衡量模型在测试集和训练集上的表现。



#### 1.3，模型选择以及交叉验证

如过只用单个j测试来检测模型的泛化能力，存在一个缺陷，单次j测试可能会对模型的泛化过于乐观（偶然性），我们还需要对模型的泛化能力进行多次验证（交叉验证）



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 195611.png)

cross validation：交叉验证集



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 195741.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 200305.png)

选择特征数量最优的模型

选择Jcv最小的模型，再用J测试进行二次验证，避免偶然性导致过拟合的情况



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 200506.png)

选择神经网络时同样适用



#### 2.1，模型评估2（偏差和方差）

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 201403.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 201523.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 201658.png)



#### 2.2，模型评估3（正则化）

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 201909.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 202030.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 202303.png)

degree of polynomial：多项式的数量



#### 2.3，制定一个性能评估的标准

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 204144.png)

语音识别例子

人类水平的表现

训练误差

交叉验证误差



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 204342.png)

建立一个性能评估标准

你能合理的希望达到什么程度的错误

1，人类水平的表现

2，竞争算法的表现

3，根据以往经验的猜测



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 204454.png)

high variance：高方差，过拟合

high bias：高偏差，欠拟合



#### 2.4，学习曲线

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 205522.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 210002.png)

在算法具有高偏差时，无论增加多少数据集，误差也不会降低



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-23 210355.png)

在算法具有高反差时，增加数据集会对你有帮助



运用学习曲线的缺点：训练的计算成本非常高



#### 2.6，神经网络与方差和偏差

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 134813.png)

tradeoff：权衡



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 140403.png)

大规模神经网络能更好地拟合复杂数据，因此在训练集上表现出较低的偏差

Jt和性能指标差距过大—>高偏差—>使用更大的神经网络—>再次检测—>Jcv和Jt差距过大—>高方差—>获取更多的数据



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 140745.png)

神经网络和正则化

只要适当选择正则化，大的神经网络往往比较小的神经网络更好

缺点：一个大的神经网络会降低你的算法的运行速度



#### 3.1，机器学习开发的迭代

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 141409.png)

机器学习开发的迭代循环

1，选择架构（模型，数据，超参数）

2，训练数据

3，诊断（偏差，方差和错误分析）

然后重新选择或调整架构，不断迭代直到满足你所需要的性能



#### 3.2，错误分析

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 144308.png)

误差分析

Mcv = 交叉验证集中的500个示例

算法错误地分类了其中的100个

手动检查100个示例，并根据共同特征对它们进行分类



例子：垃圾邮件分类

卖药（广告）：21个

故意拼写错误：3个

不寻常的电子邮件来源：7个

窃取密码(网络钓鱼)：18个

嵌入式图像中的垃圾邮件：5个



对于占比较大的比如卖药和网络钓鱼：

1，收集更多的相关数据用于训练

2，选取更合适的特征

对于占比较少的，在时间紧张的情况下可以先搁置



#### 3.3，添加更多的数据

在数据收集中，尝试获取更多所有类型的数据可能即慢又昂贵，

可以专注于添加更多分析表明可能有帮助的类型的数据（比如错误分析）

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 150553.png)

数据增强

增强：修改现有的训练示例以创建新的训练示例

例子：对字母A的识别

可以把A放大，缩小，倒过去......



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 151248.png)

通过引入失真来增强数据

更高级别的数据增强示例

取字母A并将其放在一个网格上，通过对这个网格的随机扭曲，你可以取字母A新的训练示例



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 151730.png)

数据增强对于语言识别同样适用

语音识别例子：

原始音频（语言搜索：“今天天气怎么样”）

加吵闹的背景：人群

加吵闹的背景：汽车

加手机连接不良的音频



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 152232.png)

通过引入失真来增强数据

引入的失真应该是测试集中噪音/失真类型的表示

音频：背景噪音，手机连接不良



通常对为您的数据添加纯随机/无意义的噪音通常没有帮助

例如：在图像识别中对每个像素添加噪音



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 193216.png)

数据合成：

使用人工数据输入来创建一个新的训练示例



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 193403.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 193420.png)

使用计算机文本编辑器合成的数据

合成数据的方法多用于计算机视觉当中，较少应用于其它计算机程序当中



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 193823.png)

对你的系统设计数据的使用

传统的以模型为中心的方法

以数据为中心的方法



#### 3.4，迁移学习

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 194938.png)

transfer learning：迁移学习

假设你要训练0到9手写数字的识别，但你已有的数据集不足

但你之前训练过从1百万张图片中识别出，狗，猫，人等不同的1000个类别的神经网络模型

supervised pretraining：监督预训练

fine tuning：调优

迁移学习步奏：

第一步：监督预训练（预训练）：在大型数据集上进行训练，比如1百万张不完全相关任务的图像

第二步：调优：根据从监督预训练中获取的参数和模型，然后使用你较小的数据集比如1万张手写数据集

进一步运行梯度下降以微调权重来适应你手写数字识别的特定应用



选项一：只训练输出层的参数

选项二：训练全部的参数

如果你的数据很小，推荐选项一，如果你的数据稍多，推荐选项二



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 201351.png)

迁移学习如何工作

detects edges：检测边缘

detects corners：检测角落

detects curves/basic shapes：检测曲线/基本形状

在使用相同的输入类型，比如图像检测中，神经网络一般都会先检测物体的边缘，到角落再到基本形状



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 202135.png)

迁移学习总结

1，下载与你应用程序相同的输入类型（例如图像，音频，文本）的大型数据集中预先训练的神经网络参数，或训练自己的

数据集

2，进一步用你的数据训练（微调）神经网络



#### 3.5，机器学习项目的完整周期

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 210118.png)

机器学习项目的整个周期			define：定义

第一步：确定项目的范围

定义目标

第二步：收集数据

定义并收集数据

第三步：训练模型

训练，错误分析和迭代改进

第四步：部署在生产中

部署，监控和维护系统



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-24 211027.png)

deployment：部署

inference server：推断服务器

API调用（x）

audio clip：音频剪辑

text transcript：文本记录



软件工程可能用于：

确保可靠和高效的预测

用户拓展

日志

系统监控

模型更新

MLOPS：机器学习的操作



#### 4.1，倾斜数据集的误差指标

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-25 194117.png)

罕见疾病分类示例

训练分类器Fw,b

如果存在疾病，y = 1，否则y = 0

发现在测试集上只有百分之1的误差（99%的正确率）

只有0.5的患者患这种病

accuracy：准确率		error：误差



对于倾斜数据集，假设这里有三个算法，误差分别为0.5,1,1.2很难区分哪个更好

因为误差最低的可能不是特别有用的预测，因为y预测总是为0，很难诊断出有患者患这种病



使用不同的错误度量来确定学习算法的表现如何

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-25 195253.png)

precision/recall：精确率和召回率			actual：实际的		predicted：预测的

我们想要检测的稀有类中y = 1存在

精确率：精确率关注的是模型的准确性，表示在所有预测为正的样本中，真正有多少是对的。

召回率：召回率则关注的是模型的完整性，表示在所有实际正样本中，有多少被正确识别出来。



在某些应用中，如医疗诊断或垃圾邮件过滤，可能更关注召回率，因为漏掉一个正例可能带来严重后果。

在其他场合，如金融欺诈检测，可能更关注精确率，以避免误报。



#### 4.2，精确率和召回率的权衡

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-25 202956.png)

权衡精确率和召回率		threshold：阈值

通过提高阈值可以获得更高的精确率但召回率随之降低

通过降低阈值可以获得更低的精确率但召回率随之提高



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-25 204021.png)

通过F1 score算法自动选择精确率和召回率

F1 score是一种计算平均分数的算法，它更关注较低的分数



### 第四周：机器学习中的决策树算法‘



#### 1.1，决策树模型

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-25 205318.png)

猫分类的例子

whiskers：胡须		pointy：尖的		floppy：松软的



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-25 205544.png)

root node：根节点		decision nodes：决策节点



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-25 205655.png)



#### 1.2，学习过程

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 092647.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 092827.png)

决定一：如何选择每个节点根据什么功能拆分

最大化纯度（或最小化杂质）



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 093431.png)

决定2：什么时候停止分裂

当一个节点100%是一个类时

拆分节点会导致超过你设置的最大深度时

当纯度分数提高低于阀值时（对纯度的改善太小）

当节点中的示例数量低于阀值时



如果树太大，树会变得很复杂且容易过度拟合

所以要在保证收益和保持树不至于太大之间权衡



#### 2.1，纯度

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 094218.png)

熵作为杂质的衡量标准

熵函数H（p1）越大，纯度越低

p1 = 例子中是猫的一部分



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 094451.png)

熵函数H（p1）的定义



#### 2.2，选择拆分和信息增益

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 162246.png)

选择拆分



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 162438.png)

信息增益

信息增益是用来衡量一个特征对分类任务的有效性。它通过比较使用特征前后的信息熵变化，来评估特征对数据集的“纯度”提升程度。在决策树中，信息增益越大，意味着该特征越能帮助做出准确分类。



#### 2.3，整合

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 162839.png)

决策树学习

从根节点的所有示例开始

计算所有可能特征的信息增益，并选择信息增益最高的特征

根据所选特征拆分数据集，并创建树的左右分支

继续重复拆分过程，直到满足停止标准：

当一个节点100%是一个类时

拆分节点会导致超过你设置的最大深度时

当纯度分数提高低于阀值时（对纯度的改善太小）

当节点中的示例数量低于阀值时



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 163409.png)

recursive：递归

递归拆分，递归算法



#### 2.4，独热编码one-hot

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 163744.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 163910.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 164036.png)

独热编码

如果分类特征可以采用K个值，请创建k个二进制特征（值为0或1）



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 164322.png)



#### 2.5，连续值特征的拆分

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 164619.png)



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 165004.png)

如何对连续变量进行拆分

如果你有10个示例，你将针对此阈值测试9个不同的值，然后选择为你提供信息增益最高的那一个



#### 2.6，回归树

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 165326.png)

回归决策树：预测一个数字

根据特征x来预测体重y而不是试图预测动物是否是猫



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 165618.png)

计算平均值



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-26 165757.png)

选择拆分

variance：方差

选择最大程度上能够减少方差的特征

用分裂前的均方方差减去分裂后的均方误差 = 分裂前均方误差的减少值

计算每个特征的分裂前后均方误差变化： ΔMSE=MSE（before）−MSE（after）

选择具有最大 ΔMSE的特征作为当前节点的分裂特征。即，选择能够最有效地降低预测误差的特征。



#### 3.1，使用多个决策树

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-29 164636.png)

决策树对数据的微小变化非常敏感



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-29 164806.png)

决策树组合



#### 3.2，有放回抽样

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-29 165025.png)

sampling with replacement：有放回抽样

对原始数据集进行有放回抽样建立新的数据集



#### 3.3，随机森林

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-29 165409.png)

generating a tree sample：生成树样本

给定m尺寸的训练集

对于b=1到B：

使用采样和替换来创建一个m大小的新训练集

在新数据集上训练决策树



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-29 165838.png)

随机特征选取

在每个节点上，在选择用于拆分的特征时，如果有n个特征可用，请选择k<n个特征的随机子集，

并允许算法仅从该特征的子集中进行

随机森林算法



#### 3.4，XGBoost

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-29 174804.png)

boost树 算法

给定m尺寸的训练集

对于b=1到B：

使用采样和替换来创建一个m大小的新训练集

但与其从所有概率相等（1/m）的例子中挑选，不如选择以前训练过的树木分类错误的例子

在新数据集上训练决策树



对错误的数据进行刻意练习



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-29 175235.png)

boost树的开源实现

快速高效的实施

良好的默认拆分和停止分裂的标准选择

内置正则化，以防止过度拟合

常用语机器学习竞赛中（比如：kaggle竞赛）



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-29 175801.png)



#### 3.5，何时使用决策树

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-09-29 180439.png)

决策树和神经网络算法都是非常强大的算法



决策树和树组合：

1，在表格（结构化）数据上表现更好

2，不建议在非结构化数据（图像，音频，文本）上使用决策树

3，运行更快

4，小的决策树是人类可以解释的



神经网络：

1，适用于所有类型的数据，包括表格(结构化)和非结构化数据

2，运行必决策树更慢

3，可以在工作中使用迁移学习

4，在构建一个由多个模型共同运行的系统时，将多个神经网络串起来

可能更容易





## 第三模块：无监督学习，推荐系统和强化学习

### 第一周：无监督学习



#### 1，什么是聚类

**聚类（Clustering）** 是一种无监督学习方法，其目标是将一组数据对象根据其特征或相似度划分成不同的组或簇（cluster）。每个簇内的对象在某些方面比较相似，而不同簇之间的对象则有较大的差异。

<img src="C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 134836.png" style="zoom:80%;" />



#### 2，K-means算法原理

K均值算法的伪代码：

```
text复制代码输入：数据集 X，簇数 K
1. 随机选择 K 个初始质心
2. 重复：
   1. 为每个数据点分配簇：根据质心位置，将每个数据点分配给最近的质心
   2. 更新质心：计算每个簇中所有数据点的均值，并将该均值作为新的质心
3. 直到质心不再变化，或者达到最大迭代次数
输出：K 个簇及其质心
```



<img src="C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 135225.png" style="zoom: 33%;" />

<img src="C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 135259.png" style="zoom: 33%;" />

<img src="C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 135339.png" style="zoom: 33%;" />



#### 3，K-means算法的代价函数

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 140909.png)

Ci：第 i个簇（簇的集合）

μi：第 i个簇的**质心**（即簇内所有点的均值）



K-means的代价函数（有时候也被称为失真函数）是每个训练样本xi与分配给它的聚类中心位置之间的平均平方距离



K-means的第一步：将点分配给聚类中心

保持u1到uk固定，更新c1到cm，以尽可能最小化代价函数

<img src="C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 142303.png" style="zoom: 50%;" />



K-means的第二步：移动聚类中心

保持c1到cm固定，更新u1到uk，以尽可能最小化代价函数

<img src="C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 142416.png" style="zoom: 25%;" />



#### 4，初始化K-means

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 150557.png)

随机初始化

选择K < m

随机选取k训练示例

设置u1，u2，，，uk等于这些k训练示例



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 150938.png)

多次运行初始化，计算J，并选出J最小的



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 151553.png)

随机初始化（次数最好在50到1000之间）

随机初始化K-means

运行 K-means，获取......

计算成本函数J

选择成本函数最低的



#### 5，选择聚类数量

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 152129.png)

如何选择k的值，通常，你想为以后得目的（下游）获取集群，根据K-means在后期目的上的表现进行评估



#### 6，异常检测



**异常检测**（Anomaly Detection），也叫**离群点检测**（Outlier Detection），是指识别数据集中的异常、稀有或不符合预期模式的点。异常数据点与大多数正常数据点显著不同，可能是由于错误、噪声、欺诈、故障或一些稀有的但重要的事件所引起。在很多应用场景中，及时发现这些异常点非常重要，比如信用卡欺诈检测、设备故障预测、网络安全等。



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 153241.png)

异常检测示例

飞机发动机特点：

x1 = 产生的热量

x2 = 震动强度



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 153706.png)

密度估计：

首先建立一个x的概率模型（学习算法将尝试找出特征x1和x2中的哪些值具有高概率，以及哪些值不太可能或具有较低的概率出现。）

当你学习了一个p（x）后，当你给出一个新的测试样本x_test后，你将计算x_test的概率

如果p（x）小于阈值，则可能异常



#### 7，高斯正态分布

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 201258.png)

高斯（正态）分布：

x的概率由均值u，方差o2决定



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 201514.png)

方差越小，图像越陡，概率之和为1



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 201740.png)

最大似然估计



#### 8，异常检测算法

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 202033.png)

每个例子有n个特征



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 202330.png)

异常检测算法

1，选择n个特征x，你认为可能表明异常的示例

2，拟合参数

3，给定新的测试集



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 202752.png)



#### 9，开发和评估异常检测系统

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 203023.png)

实数评估的重要性：

在开发学习算法（选择特征）时，

如果我们有办法评估我们的学习算法，做出决定就容易多了

假设我们有一些标注的数据，包括异常和非异常的例子

Y=1：表示异常

Y=0：表示正常

训练集：假设全部都是正常而非异常的

在交叉验证集和测试集中，添加少量的异常的例子，大部分任然是正常的例子



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 203829.png)

查看数据集在交叉验证集和测试集上的正确率来调整参数和增删特征



#### 10，异常检测和监督学习的对比

在监督学习中我们倾向于假设未来的正样本（Y=1）与训练集中的正样本相似,，可以从先存的正样本中学习到（比如垃圾邮件的检测）

当异常的种类很多，且对于未来发生的异常难以预测，很难从现存的正样本中学习到，就使用异常检测（比如金融诈骗）



#### 11，特征的选择

因为在无监督学习中没有标签，所以特征的选择也更加重要

![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 205016.png)

如果你的图像不是高斯分布，利用绘图将其变换为高斯分布



![](C:\Users\23012\OneDrive\Pictures\Screenshots\屏幕截图 2024-11-22 205602.png)

训练一个模型，然后查看算法在交叉验证集中未能检测到的异常，然后

查看这些样本，看看是否能启发创建新的特征，使算法能够识别出该样本的异常

