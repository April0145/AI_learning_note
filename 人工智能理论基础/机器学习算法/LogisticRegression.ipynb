{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2uvnZ7GphUAAygDQH1AtS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["逻辑回归"],"metadata":{"id":"IfrERfd0a7UC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVdvkcidapon"},"outputs":[],"source":["# 手动实现逻辑回归\n","import numpy as np\n","\n","# Sigmoid 函数\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","def compute_cost(y, y_pred):\n","    \"\"\"计算逻辑回归的成本函数\"\"\"\n","    m = len(y)\n","    cost = - (1 / m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n","    return cost\n","\n","def train_logistic_regression(X, y, learning_rate=0.01, num_iterations=1000, lambda_ = 0.1):\n","    # X.shape：获取特征矩阵X的形状，m是样本数量，n是特征数量。\n","    # weights：初始化为全零的权重向量。\n","    # bias：初始化为零的偏置项。\n","    m, n = X.shape\n","    weights = np.zeros(n)  # 初始化权重\n","    bias = 0  # 初始化偏置\n","\n","    for i in range(num_iterations): # 进行num_iterations次迭代。\n","        # 计算预测值\n","        z = np.dot(X, weights) + bias\n","        y_pred = sigmoid(z)\n","\n","        # 计算损失\n","        loss = compute_cost(y, y_pred)\n","\n","        # 计算梯度\n","        dz = y_pred - y\n","        dw = np.dot(X.T, dz) / m\n","        db = np.sum(dz) / m\n","\n","        # 更新权重和偏置\n","        weights -= learning_rate * (dw + lambda_/m * weights) # 使用学习率更新权重\n","        bias -= learning_rate * db # 使用学习率更新偏置。\n","\n","        if i % 100 == 0:  # 每 100 次迭代打印损失\n","            print(f\"Iteration {i}: Loss = {loss}\")\n","\n","    return weights, bias\n","\n","# 预测函数\n","def predict(X, weights, bias):\n","    z = np.dot(X, weights) + bias\n","    y_pred = sigmoid(z)\n","    return np.round(y_pred)"]}]}