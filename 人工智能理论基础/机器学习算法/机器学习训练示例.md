# 机器学习训练示例



### 一，数据采集与处理

#### 1，下载或采集数据

#### 2，导入数据

第一步：将数据上传到Google drive（谷歌硬盘）

第二步：查看该目录下文件，确保数据上传成功

```
import os
# 列出目录中的文件
dir_path = '/content/drive/MyDrive/目录/目录'
files = os.listdir(dir_path)
print(files)  # 打印目录中的文件
```

第三步：将 Google Drive中的数据 挂载到 Goole Colab 环境中

```
drive.mount('/content/drive')
my_data = pd.read_csv('/content/drive/MyDrive/目录/目录/数据集')
```

#### 3，数据清洗

##### （1），数据概览

```
#查看前五行
mydata.head()
#查看数据类型和缺失值信息
mydata.info()
#查看数据统计
mydata.describe()
```



##### （2），处理缺失值

查找缺失值的位置和数量

```
print(df.isnull().sum())
```

对于缺失值的处理方式：

one:使用值来填充（破坏性低）

```
def data_miss(titanic, column_1):
  # 前向填充
  titanic[[column_1]] = titanic[[column_1]].fillna(method='ffill')
  return titanic
```



```
#使用常数填充: 将缺失值用某个常数（例如0、-9999等）填充。
df.fillna(0, inplace=True)

#均值填充: 对于数值型数据，可以用该列的均值填充缺失值。
df.fillna(df.mean(), inplace=True)

#中位数填充: 如果数据分布不对称，中位数是一个更稳健的选择。
df.fillna(df.median(), inplace=True)

#众数填充: 对于分类数据，可以用最常出现的值（众数）填充缺失值。
df.fillna(df.mode().iloc[0], inplace=True)

#前向填充: 用前一个有效值填充缺失值。
df.fillna(method='ffill', inplace=True)

#后向填充: 用后一个有效值填充缺失值。
df.fillna(method='bfill', inplace=True)

#线性插值: 基于数据的趋势来插值缺失值。适用于有序的数据。
df.interpolate(method='linear', inplace=True)

```

two:删除包含缺失值的行或列

```
#删除行: 如果某些行的缺失值非常多，或者这些行对分析的影响不大，可以选择删除这些行。
df.dropna(inplace=True)

#删除列: 如果某些列的缺失值非常多，且对分析不重要，可以选择删除这些列。
df.dropna(axis=1, inplace=True)
```



#####  (3)，处理重复数据

```
#查找重复数据: 识别重复的行。
print(df.duplicated().sum())

#删除重复数据: 删除重复的行。
df.drop_duplicates(inplace=True)
```



##### （4），处理分类数据

查看类别

```
#对于分类型数据可以查看数据类别
mydata.value_counts()
```



**独热编码（One-Hot Encoding）**：独热编码（One-Hot Encoding）是一种将分类数据转换为数值数据的常用方法，特别是在处理机器学习模型时。它将每个类别表示为一个二进制向量，其中只有一个元素为 1，其余元素为 0。

许多机器学习模型（如线性回归、逻辑回归、决策树等）无法直接处理分类数据，因为它们只能处理数值数据。独热编码将分类变量转换为数值形式，使模型能够处理这些数据。

假设有一个包含三个类别的变量 `Color`，其可能的取值为 `Red`、`Green` 和 `Blue`。独热编码将这些类别转换为以下形式的二进制向量：

- **Red** → [1, 0, 0]
- **Green** → [0, 1, 0]
- **Blue** → [0, 0, 1]

每个类别都被转换成一个长度为 3 的向量，其中对应类别的位置为 1，其余位置为 0。

参数**`sparse=True`（默认值）**：输出一个稀疏矩阵。这种格式在处理大量数据时节省内存，因为它只存储非零元素。

**`sparse=False`**：输出一个密集矩阵，即普通的 NumPy 数组。这种格式可能占用更多内存，但在处理较小数据集时更易于操作和查看。

```
def data_code(titanic, column_1):
# 初始化 OneHotEncoder，设置 sparse=False 以返回稠密数组
  encoder = OneHotEncoder(sparse=False)

# 对 DataFrame 的分类列进行独热编码
  titanic_cat = encoder.fit_transform(titanic[[column_1]])

# 将编码后的数据转换为 DataFrame，并设置列名
  df_encoded = pd.DataFrame(titanic_cat, columns=encoder.get_feature_names_out([column_1]))

# 将编码后的 DataFrame 与原始 DataFrame 进行合并，并删除原始的分类列
  titanic = pd.concat([titanic, df_encoded], axis=1).drop(column_1, axis=1)
  return titanic
titanic = data_code(titanic, 'Sex')
titanic = data_code(titanic, 'Embarked')
titanic.head()
```



#### 4，特征处理

##### （1），特征缩放

标准化：

```
# 使用 scikit-learn 标准化
def feature_scaling(titanic, column_1):
  scaler = StandardScaler()
  tn_age = titanic[[column_1]]
  df_scaled = pd.DataFrame(scaler.fit_transform(tn_age), columns=tn_age.columns)
  titanic = titanic.drop(column_1, axis=1)
  titanic = pd.concat([titanic, df_scaled], axis=1)
  return titanic
titanic = feature_scaling(titanic, 'Age')
titanic = feature_scaling(titanic, 'Fare')
titanic.head()
```



```
# 手动标准化
mean = titanic['Age'].mean()
std = titanic['Age'].std()

tn_age = titanic[['Age']]
Standardized_Age = (tn_age - mean) / std
df_scaled = pd.DataFrame(Standardized_Age, columns=tn_age.columns)

titanic = titanic.drop('Age', axis=1)
titanic = pd.concat([titanic, df_scaled], axis=1)
titanic.head()
```



##### （2），特征选取



#### 5，创建测试集

将数据集划分为测试集和训练集（如果数据集没有提前划分的话）

测试集通常为数据集的20%，如果你的数据集非常大则更少。

`scikit-learn` 提供了一个非常方便的函数 `train_test_split` 来实现数据集的划分。

```
#举例
import pandas as pd
from sklearn.model_selection import train_test_split

# 读取数据集
titanic = pd.read_csv('/content/drive/MyDrive/kaggle/titanic/train.csv')

# 假设 'Survived' 是目标变量，其他列是特征
X = titanic.drop('Survived', axis=1)  # 特征
y = titanic['Survived']  # 目标变量

# 划分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# test_size=0.2 表示将 20% 的数据用作测试集，80% 用作训练集
# random_state 是一个随机种子，确保每次运行时结果一致

```

**`random_state`**: 随机种子，用于确保每次运行时数据划分的一致性。设定一个整数值即可。

**`test_size`**: 测试集的比例或绝对数量。可以是浮点数（如 0.2 表示 20%）或整数（如 100 表示 100 个测试样本）。默认为 `None`，如果 `train_size` 为 `None`，则测试集比例为 0.25。

注意事项：在划分数据集之前，通常需要对数据进行预处理，例如处理缺失值、编码分类特征等



#### 6，模型选择

```
# 使用逻辑回归模型
# 创建并训练模型
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# 这里创建了一个逻辑回归模型的实例。penalty='l2'表示使用L2正则化，这是为了避免过拟合并提高模型的泛化能力。C=1.0是正则化参数，控制正则化的强度。较大的C值对应于较弱的正则化，较小的C值对应于较强的正则化。
model = LogisticRegression(penalty='l2', C=1.0) # 使用正则化
model.fit(x_train, y_train)
```



#### 7，预测

```
# 对新的数据集进行预测
# 测试数据
X_new = titanic_1

# 预测
y_pred = model.predict(X_new)

print("预测结果:", y_pred)
```









```
# kaggle文件查询
import os

 = '/kaggle/input/'
for dirname, _, filenames in os.walk(input_dir):
    for filename in filenames:
        print(os.path.join(dirname, filename))
```

