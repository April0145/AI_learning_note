# Langchainåº“çš„ä½¿ç”¨

## 1ï¼Œè°ƒç”¨æ¨¡å‹

ï¼ˆ1ï¼‰å¯¼å…¥æ¨¡å—

```
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv, find_dotenv
```

ï¼ˆ2ï¼‰åˆ›å»º`.env`æ–‡ä»¶ï¼Œå­˜å‚¨ç§˜é’¥

```
DEEPSEEK_API_KEY=**************
```

ï¼ˆ3ï¼‰è°ƒç”¨æ¨¡å‹

```
# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv(find_dotenv())

# åˆ›å»º ChatOpenAI å®ä¾‹å¹¶ä¼ å…¥ API å¯†é’¥
chat = ChatOpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com/v1",
    temperature=0.0, # è¶Šä½è¶Šä¸¥è°¨ï¼Œè¶Šé«˜è¶Šå¼€æ”¾
    model='deepseek-chat'
)
```

ï¼ˆ4ï¼‰å®Œæ•´ç¤ºä¾‹

```
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv, find_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv(find_dotenv())

# åˆ›å»º ChatOpenAI å®ä¾‹å¹¶ä¼ å…¥ API å¯†é’¥
chat = ChatOpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com/v1",
    temperature=0.0,
    model='deepseek-chat'
)

message = ("å†™ä¸€é¦–æ­Œ")

response = chat.invoke(message)  # invoke()ä¼šå°†è¾“å…¥çš„æ¶ˆæ¯åˆ—è¡¨ä¼ é€’ç»™æ¨¡å‹ï¼Œè¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆä¸€ä¸ªå“åº”ã€‚
print(response.content)  # .content å±æ€§è·å–æœ€ç»ˆçš„æ–‡æœ¬è¾“å‡º
```



## 2,æç¤ºæ¨¡ç‰ˆ

### (1),PromptTemplate

ï¼ˆ1ï¼‰å¯¼å…¥æ¨¡å—

```
from langchain.prompts import PromptTemplate
```

ï¼ˆ2ï¼‰æ„é€ æç¤ºè¯æ¨¡ç‰ˆ

```
# å®šä¹‰ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿ï¼ŒåŠ¨æ€æ„é€ è¾“å…¥
prompt = PromptTemplate(
    input_variables=["theme"], # è¾“å…¥å˜é‡
    template="è¯·å†™ä¸€é¦–å…³äº{theme}çš„è¯—" # æ¨¡ç‰ˆ
)
```

ï¼ˆ3ï¼‰ç”¨æˆ·è¾“å…¥

```
# ç”¨æˆ·ç»™å‡ºä¸€ä¸ªä¸»é¢˜
user_theme = input("è¯·è¾“å…¥ä½ æƒ³è¦çš„è¯—æ­Œä¸»é¢˜ï¼š")
```

ï¼ˆ4ï¼‰ç”¨æ¨¡ç‰ˆç”Ÿæˆæç¤ºè¯

```
# ç”¨æ¨¡æ¿ç”Ÿæˆæç¤ºè¯
message = prompt.format(theme=user_theme)
```

ï¼ˆ5ï¼‰ç¤ºä¾‹

```
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv, find_dotenv
from langchain.prompts import PromptTemplate

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv(find_dotenv())

# åˆ›å»º ChatOpenAI å®ä¾‹å¹¶ä¼ å…¥ API å¯†é’¥
chat = ChatOpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com/v1",
    temperature=0.0,
    model='deepseek-chat'
)

# å®šä¹‰ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿
prompt = PromptTemplate(
    input_variables=["theme"],
    template="è¯·å†™ä¸€é¦–å…³äº{theme}çš„è¯—"
)

# ç”¨æˆ·ç»™å‡ºä¸€ä¸ªä¸»é¢˜
user_theme = input("è¯·è¾“å…¥ä½ æƒ³è¦çš„è¯—æ­Œä¸»é¢˜ï¼š")

# ç”¨æ¨¡æ¿ç”Ÿæˆæç¤ºè¯
message = prompt.format(theme=user_theme)

response = chat.invoke(message)  # invoke() ä¼šå°†è¾“å…¥çš„æ¶ˆæ¯åˆ—è¡¨ä¼ é€’ç»™æ¨¡å‹ï¼Œè¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆä¸€ä¸ªå“åº”ã€‚
print(response.content)  # .content å±æ€§è·å–æœ€ç»ˆçš„æ–‡æœ¬è¾“å‡º
```

### (2),èŠå¤©æç¤ºæ¨¡ç‰ˆ-ChatPromptTemplate

- **ç”¨é€”**ï¼šä¸“ä¸ºèŠå¤©æ¨¡å‹ï¼ˆå¦‚ GPT-4ã€DeepSeek Chatï¼‰è®¾è®¡ã€‚
- **è¾“å‡º**ï¼šç”Ÿæˆç»“æ„åŒ–æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«è§’è‰²æ ‡è®°ï¼Œå¦‚ `system`/`user`/`ai`ï¼‰

ï¼ˆ1ï¼‰å¯¼å…¥

```
from langchain.prompts import ChatPromptTemplate
```

ï¼ˆ2ï¼‰æ„é€ èŠå¤©æç¤ºè¯æ¨¡ç‰ˆ

```
# åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä½æ‰åæ¨ªæº¢çš„è¯—äºº"),
    ("user", "è¯·å†™ä¸€é¦–å…³äº{theme}çš„è¯—")
])
```

ï¼ˆ3ï¼‰ç¤ºä¾‹

```
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from dotenv import load_dotenv, find_dotenv
import os

# è½½å…¥ API å¯†é’¥
load_dotenv(find_dotenv())
api_key = os.getenv("DEEPSEEK_API_KEY")

# åˆ›å»ºæ¨¡å‹
chat = ChatOpenAI(
    api_key=api_key,
    base_url="https://api.deepseek.com/v1",
    temperature=0.5,
    model="deepseek-chat"
)

# åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([ # from_messageså¤šæ¶ˆæ¯ç»“æ„åŒ–æç¤º
    ("system", "ä½ æ˜¯ä¸€ä½æ‰åæ¨ªæº¢çš„è¯—äºº"),
    ("user", "è¯·å†™ä¸€é¦–å…³äº{theme}çš„è¯—")
])


# ä½¿ç”¨|è¿æ¥æç¤ºå’Œæ¨¡å‹
poem_chain = prompt | chat

# ç”¨æˆ·è¾“å…¥
user_input = input("è¯·è¾“å…¥è¯—çš„ä¸»é¢˜ï¼š")

# è°ƒç”¨é“¾ï¼Œä¼ å…¥å˜é‡å­—å…¸ï¼Œè‡ªåŠ¨æ„å»ºæç¤º + è°ƒç”¨æ¨¡å‹
response = poem_chain.invoke({"theme": user_input})

# è¾“å‡ºç»“æœ
print("\nğŸŒ¸ AI ä¸ºä½ å†™çš„è¯—ï¼š\n")
print(response.content)
```





## 3ï¼Œé“¾ 

### (1),Runnable

ï¼ˆ1ï¼‰æ— éœ€æ˜¾å¼è°ƒç”¨

ï¼ˆ2ï¼‰ä½¿ç”¨|é“¾æ¥

```
poem_chain = prompt | chat
```

ï¼ˆ3ï¼‰è°ƒç”¨é“¾ï¼Œä¼ å…¥å˜é‡å­—å…¸ï¼Œè‡ªåŠ¨æ„å»ºæç¤º + è°ƒç”¨æ¨¡å‹

```
response = poem_chain.invoke({"theme": user_input})
```

ï¼ˆ4ï¼‰ç¤ºä¾‹

```
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv, find_dotenv
import os

# è½½å…¥ API å¯†é’¥
load_dotenv(find_dotenv())
api_key = os.getenv("DEEPSEEK_API_KEY")

# åˆ›å»ºæ¨¡å‹
chat = ChatOpenAI(
    api_key=api_key,
    base_url="https://api.deepseek.com/v1",
    temperature=0.5,
    model="deepseek-chat"
)

# åˆ›å»ºæç¤ºæ¨¡æ¿
prompt = PromptTemplate(
    input_variables=["theme"],
    template="è¯·å†™ä¸€é¦–å…³äº{theme}çš„è¯—"
)

# ä½¿ç”¨|è¿æ¥æç¤ºå’Œæ¨¡å‹
poem_chain = prompt | chat

# ç”¨æˆ·è¾“å…¥
user_input = input("è¯·è¾“å…¥è¯—çš„ä¸»é¢˜ï¼š")

# è°ƒç”¨é“¾ï¼Œä¼ å…¥å˜é‡å­—å…¸ï¼Œè‡ªåŠ¨æ„å»ºæç¤º + è°ƒç”¨æ¨¡å‹
response = poem_chain.invoke({"theme": user_input})

# è¾“å‡ºç»“æœ
print("\nğŸŒ¸ AI ä¸ºä½ å†™çš„è¯—ï¼š\n")
print(response.content)
```

### (2),ç›´é€šé“¾-RunnablePassthrough

ï¼ˆ1ï¼‰å¯¼å…¥æ¨¡å—

```
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
```

ï¼ˆ2ï¼‰æ„å»ºpromptæ¨¡å—

```
# Prompt æ¨¡æ¿
prompt1 = ChatPromptTemplate.from_template("å°†ä»¥ä¸‹æ–‡æœ¬å†…å®¹ç¿»è¯‘ä¸ºç®€ä½“ä¸­æ–‡: {text}")
prompt2 = ChatPromptTemplate.from_template("ç”¨10å­—ä»¥å†…çš„ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹æ–‡æœ¬: {æ–‡æœ¬ç¿»è¯‘}")
prompt3 = ChatPromptTemplate.from_template("æ ¹æ®æ€»ç»“å†™ä¸€ä¸ªç®€çŸ­å›å¤ï¼Œæ€»ç»“: {æ–‡æœ¬æ€»ç»“}")
```

ï¼ˆ3ï¼‰æ„å»ºé“¾

```
# æ„å»º LCEL é“¾ï¼Œå…¨æ˜¾ç¤ºå†™æ³•ï¼Œä¸æ¨è
runnable_chain = RunnablePassthrough.assign(
    æ–‡æœ¬ç¿»è¯‘=(itemgetter("text") | prompt1 | chat | StrOutputParser())
).assign(
    æ–‡æœ¬æ€»ç»“=(itemgetter("æ–‡æœ¬ç¿»è¯‘") | prompt2 | chat | StrOutputParser())
).assign(
    å›å¤=( itemgetter("æ–‡æœ¬æ€»ç»“") | prompt3 | chat | StrOutputParser())
)

# æ„å»º LCELé“¾ï¼Œä¾èµ–è‡ªåŠ¨åŒ¹é…
runnable_chain = RunnablePassthrough.assign(
    æ–‡æœ¬ç¿»è¯‘= prompt1 | chat | StrOutputParser()
).assign(
    æ–‡æœ¬æ€»ç»“= prompt2 | chat | StrOutputParser()
).assign(
    å›å¤= prompt3 | chat | StrOutputParser()
)
```

`assign` æ–¹æ³•ç”¨äºå‘è¾“å…¥å­—å…¸ä¸­æ·»åŠ æ–°çš„é”®å€¼å¯¹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸º `æ–‡æœ¬ç¿»è¯‘` çš„æ–°é”®ã€‚ç­‰å·å³è¾¹çš„éƒ¨åˆ†å®šä¹‰äº†å¦‚ä½•ç”Ÿæˆè¿™ä¸ªé”®å¯¹åº”çš„å€¼ï¼š

- `itemgetter("text")`: ä»è¾“å…¥å­—å…¸ `{"text": ...}` ä¸­æå– `text` çš„å€¼ã€‚
- `| prompt1`: å°†æå–çš„ `text` ä¼ é€’ç»™ `prompt1` è¿›è¡Œæ ¼å¼åŒ–ã€‚
- `| chat`: å°†æ ¼å¼åŒ–åçš„ prompt å‘é€ç»™ `chat` æ¨¡å‹è·å–å›å¤ã€‚
- `| StrOutputParser()`: å°† `chat` æ¨¡å‹çš„è¾“å‡ºè§£ææˆä¸€ä¸ªå­—ç¬¦ä¸²ã€‚ å› æ­¤ï¼Œ`æ–‡æœ¬ç¿»è¯‘` çš„å€¼å°±æ˜¯åŸå§‹æ–‡æœ¬çš„ç®€ä½“ä¸­æ–‡ç¿»è¯‘ã€‚
- `prompt3` çš„æ¨¡æ¿å˜é‡å `{æ–‡æœ¬æ€»ç»“}`
  ä¼šè‡ªåŠ¨åŒ¹é…å­—å…¸ä¸­çš„åŒåå­—æ®µï¼ˆLangChain çš„æ™ºèƒ½ç‰¹æ€§ï¼‰

ï¼ˆ4ï¼‰å®Œæ•´ç¤ºä¾‹

```
import os
import time
from dotenv import load_dotenv, find_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# åŸºæœ¬è®¾ç½®
load_dotenv(find_dotenv())
chat = ChatOpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com/v1",
    temperature=0.0,
    model='deepseek-reasoner'
)

# Prompt æ¨¡æ¿
prompt1 = ChatPromptTemplate.from_template("å°†ä»¥ä¸‹æ–‡æœ¬å†…å®¹ç¿»è¯‘ä¸ºç®€ä½“ä¸­æ–‡: {text}")
prompt2 = ChatPromptTemplate.from_template("ç”¨10å­—ä»¥å†…çš„ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹æ–‡æœ¬: {æ–‡æœ¬ç¿»è¯‘}")
prompt3 = ChatPromptTemplate.from_template("ä½œä¸ºå­¦ç”Ÿæ ¹æ®æ€»ç»“å†™ä¸€ä¸ªç®€çŸ­å›å¤ï¼Œæ€»ç»“: {æ–‡æœ¬æ€»ç»“}")

# æ„å»º LCEL é“¾
runnable_chain = RunnablePassthrough.assign(
    æ–‡æœ¬ç¿»è¯‘= prompt1 | chat | StrOutputParser()
).assign(
    æ–‡æœ¬æ€»ç»“= prompt2 | chat | StrOutputParser()
).assign(
    å›å¤= prompt3 | chat | StrOutputParser()
)

# è¾“å…¥æ–‡æœ¬
text = ("I won't approve any leave requests for volunteering during the seventh and eighth periods. We really can't manage without you. If it's absolutely "
        "necessary, have the teacher in charge contact me directly, or ask the supervising teacher there to approve your leave slip")

# æ‰§è¡Œä¸è®¡æ—¶
start_time = time.time()
result = runnable_chain.invoke({"text": text})
end_time = time.time()
t = end_time - start_time

# æ‰“å°ç»“æœ
print(f"ç¿»è¯‘ç»“æœï¼š\n{result['æ–‡æœ¬ç¿»è¯‘']}")
print(f"\næ–‡æœ¬æ€»ç»“ï¼š\n{result['æ–‡æœ¬æ€»ç»“']}")
print(f"\nç”Ÿæˆçš„å›å¤ï¼š\n{result['å›å¤']}")
print(f"\nå“åº”æ—¶é—´ï¼š\n{t:.4f} ç§’")
```

ï¼ˆ5ï¼‰éé¡ºåºé“¾ç¤ºä¾‹

```
import os
import time
from dotenv import load_dotenv, find_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# åŸºæœ¬è®¾ç½®
load_dotenv(find_dotenv())
chat = ChatOpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com/v1",
    temperature=0.0,
    model='deepseek-chat'
)

# Prompt æ¨¡æ¿
prompt1 = ChatPromptTemplate.from_template("å°†{text}ç¿»è¯‘ä¸ºç®€ä½“ä¸­æ–‡ï¼Œå›å¤çš„æ—¶å€™å°½é‡ç®€æ´") #from_templateå•æ¶ˆæ¯ç»“æ„åŒ–æç¤º
prompt2 = ChatPromptTemplate.from_template("ç”¨ä¸€å¥è¯æ€»ç»“{æ–‡æœ¬ç¿»è¯‘}ï¼Œå›å¤çš„æ—¶å€™å°½é‡ç®€æ´")
prompt3 = ChatPromptTemplate.from_template("åˆ†æ{æ–‡æœ¬ç¿»è¯‘}ä¸­çš„æƒ…ç»ªï¼Œç”¨ä¸¤ä¸ªä¸­æ–‡å•è¯è¡¨ç¤ºï¼Œå›å¤çš„æ—¶å€™å°½é‡ç®€æ´")
prompt4 = ChatPromptTemplate.from_template("å‡è®¾ä½ æ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œæ ¹æ®{æ–‡æœ¬ç¿»è¯‘}å’Œ{æƒ…ç»ªæ£€æµ‹}ä¸­çš„å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„å›å¤ï¼Œå›å¤çš„æ—¶å€™å°½é‡ç®€æ´")

# æ„å»º LCEL é“¾
runnable_chain = RunnablePassthrough.assign(
    æ–‡æœ¬ç¿»è¯‘= prompt1 | chat | StrOutputParser()
).assign(
    æ–‡æœ¬æ€»ç»“= prompt2 | chat | StrOutputParser()
).assign(
    æƒ…ç»ªæ£€æµ‹= prompt3 | chat | StrOutputParser()
).assign(
    å›å¤= prompt4 | chat | StrOutputParser()
)

# è¾“å…¥æ–‡æœ¬
text = ("I won't approve any leave requests for volunteering during the seventh and eighth periods. We really can't manage without you. If it's absolutely "
        "necessary, have the teacher in charge contact me directly, or ask the supervising teacher there to approve your leave slip")

# æ‰§è¡Œä¸è®¡æ—¶
start_time = time.time()
result = runnable_chain.invoke({"text": text})
end_time = time.time()
t = end_time - start_time

# æ‰“å°ç»“æœ
print(f"ç¿»è¯‘ç»“æœï¼š\n{result['æ–‡æœ¬ç¿»è¯‘']}")
print(f"\næ–‡æœ¬æ€»ç»“ï¼š\n{result['æ–‡æœ¬æ€»ç»“']}")
print(f"\næƒ…ç»ªåˆ†æï¼š\n{result['æƒ…ç»ªæ£€æµ‹']}")
print(f"\nç”Ÿæˆçš„å›å¤ï¼š\n{result['å›å¤']}")
print(f"\nå“åº”æ—¶é—´ï¼š\n{t:.4f} ç§’")
```

### (3),å¹¶è¡Œé“¾-RunnableParallel

å¹¶è¡Œå¤„ç†å¤šä¸ªç›¸äº’ç‹¬ç«‹çš„ä»»åŠ¡ï¼Œæ‰€æœ‰ä»»åŠ¡æ¥å—ç›¸åŒçš„åŸå§‹è¾“å…¥ï¼Œä¸ä¿ç•™ä¸­é—´æ•°æ®

```
import os
import time
from dotenv import load_dotenv, find_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel

# åŸºæœ¬è®¾ç½®
load_dotenv(find_dotenv())
chat = ChatOpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com/v1",
    temperature=0.0,
    model='deepseek-reasoner'
)

# Prompt æ¨¡æ¿
prompt1 = ChatPromptTemplate.from_template("åˆ†æ{text}ä¸­çš„æƒ…ç»ªï¼Œç”¨ä¸¤ä¸ªä¸­æ–‡å•è¯è¡¨ç¤ºï¼Œå›å¤çš„æ—¶å€™å°½é‡ç®€æ´") #from_templateå•æ¶ˆæ¯ç»“æ„åŒ–æç¤º
prompt2 = ChatPromptTemplate.from_template("æ ¹æ®{text}å†™ä¸€ä¸ª20å­—ä»¥å†…çš„æ–‡æœ¬æ€»ç»“ï¼Œå›å¤çš„æ—¶å€™å°½é‡ç®€æ´")
prompt3 = ChatPromptTemplate.from_template("ä½œä¸ºå­¦ç”Ÿæ ¹æ®{text}å†™ä¸€ä¸ªç®€çŸ­çš„å›å¤ï¼Œå›å¤çš„æ—¶å€™å°½é‡ç®€æ´")

# æ„å»º LCEL é“¾
runnable_chain = RunnableParallel(
    æ–‡æœ¬æ€»ç»“= prompt2 | chat | StrOutputParser(),
    æƒ…ç»ªæ£€æµ‹= prompt1 | chat | StrOutputParser(),
    å›å¤= prompt3 | chat | StrOutputParser()
)

# è¾“å…¥æ–‡æœ¬
text = ("ä»Šå¤©ä¸­å›½æ–‡åŒ–å…±åŒä½“ç¼ºæ—·çš„å­¦ç”Ÿï¼Œä»Šå¤©ä¹‹å†…ç§èŠæˆ‘å‘Šè¯‰æˆ‘åŸå› ã€‚å¹¶ä¸”äºæ¯äººä¸‹å‘¨ä¹‹å‰å†™ä¸€ä»½1000å­—æ£€è®¨äº¤ç»™æˆ‘ã€‚@å…¨ä½“æˆå‘˜")

# æ‰§è¡Œä¸è®¡æ—¶
start_time = time.time()
result = runnable_chain.invoke({"text": text})
end_time = time.time()
t = end_time - start_time

# æ‰“å°ç»“æœ
print(f"\næ–‡æœ¬æ€»ç»“ï¼š\n{result['æ–‡æœ¬æ€»ç»“']}")
print(f"\næƒ…ç»ªåˆ†æï¼š\n{result['æƒ…ç»ªæ£€æµ‹']}")
print(f"\nç”Ÿæˆçš„å›å¤ï¼š\n{result['å›å¤']}")
print(f"\nå“åº”æ—¶é—´ï¼š\n{t:.4f} ç§’")
```

### (4),è·¯ç”±(åˆ†ç±»)é“¾â€”RunnableLambda

`RunnableLambda` æ˜¯ `langchain` ä¸­çš„ä¸€ç§æ„é€ ï¼Œå®ƒå…è®¸ä½ å°†å‡½æ•°åŒ…è£…æˆä¸€ä¸ªå¯æ‰§è¡Œçš„å¯è°ƒç”¨å¯¹è±¡ï¼Œä½¿å¾—è¿™ä¸ªå‡½æ•°å¯ä»¥ä½œä¸ºä¸€ä¸ªâ€œå¯è¿è¡Œçš„é“¾â€æ¥è°ƒç”¨ã€‚

ï¼ˆ1ï¼‰ï¼Œå®šä¹‰æ¨¡ç‰ˆ,éœ€è¦æœ‰ä¸€ä¸ªé»˜è®¤æ¨¡ç‰ˆ

```
# 2ï¼Œå®šä¹‰æ¨¡ç‰ˆ

physics_template = """ä½ æ˜¯ä¸€ä¸ªéå¸¸èªæ˜çš„ç‰©ç†ä¸“å®¶ã€‚ \
ä½ æ“…é•¿ç”¨ä¸€ç§ç®€æ´å¹¶ä¸”æ˜“äºç†è§£çš„æ–¹å¼å»å›ç­”é—®é¢˜ã€‚\
å½“ä½ ä¸çŸ¥é“é—®é¢˜çš„ç­”æ¡ˆæ—¶ï¼Œä½ æ‰¿è®¤\
ä½ ä¸çŸ¥é“.

å…·ä½“é—®é¢˜å¦‚ä¸‹:
{input}"""

math_template = """ä½ æ˜¯ä¸€ä¸ªéå¸¸ä¼˜ç§€çš„æ•°å­¦å®¶ã€‚ \
ä½ æ“…é•¿å›ç­”æ•°å­¦é—®é¢˜ã€‚ \
ä½ ä¹‹æ‰€ä»¥å¦‚æ­¤ä¼˜ç§€ï¼Œ \
æ˜¯å› ä¸ºä½ èƒ½å¤Ÿå°†æ£˜æ‰‹çš„é—®é¢˜åˆ†è§£ä¸ºç»„æˆéƒ¨åˆ†ï¼Œ\
å›ç­”ç»„æˆéƒ¨åˆ†ï¼Œç„¶åå°†å®ƒä»¬ç»„åˆåœ¨ä¸€èµ·ï¼Œå›ç­”æ›´å¹¿æ³›çš„é—®é¢˜ã€‚

å…·ä½“é—®é¢˜å¦‚ä¸‹ï¼š
{input}"""

history_template = """ä½ æ˜¯ä»¥ä¸ºéå¸¸ä¼˜ç§€çš„å†å²å­¦å®¶ã€‚ \
ä½ å¯¹ä¸€ç³»åˆ—å†å²æ—¶æœŸçš„äººç‰©ã€äº‹ä»¶å’ŒèƒŒæ™¯æœ‰ç€æå¥½çš„å­¦è¯†å’Œç†è§£\
ä½ æœ‰èƒ½åŠ›æ€è€ƒã€åæ€ã€è¾©è¯ã€è®¨è®ºå’Œè¯„ä¼°è¿‡å»ã€‚\
ä½ å°Šé‡å†å²è¯æ®ï¼Œå¹¶æœ‰èƒ½åŠ›åˆ©ç”¨å®ƒæ¥æ”¯æŒä½ çš„è§£é‡Šå’Œåˆ¤æ–­ã€‚

å…·ä½“é—®é¢˜å¦‚ä¸‹:
{input}"""

computer_template = """ ä½ æ˜¯ä¸€ä¸ªæˆåŠŸçš„è®¡ç®—æœºç§‘å­¦ä¸“å®¶ã€‚\
ä½ æœ‰åˆ›é€ åŠ›ã€åä½œç²¾ç¥ã€\
å‰ç»æ€§æ€ç»´ã€è‡ªä¿¡ã€è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€\
å¯¹ç†è®ºå’Œç®—æ³•çš„ç†è§£ä»¥åŠå‡ºè‰²çš„æ²Ÿé€šæŠ€å·§ã€‚\
ä½ éå¸¸æ“…é•¿å›ç­”ç¼–ç¨‹é—®é¢˜ã€‚\
ä½ ä¹‹æ‰€ä»¥å¦‚æ­¤ä¼˜ç§€ï¼Œæ˜¯å› ä¸ºä½ çŸ¥é“  \
å¦‚ä½•é€šè¿‡ä»¥æœºå™¨å¯ä»¥è½»æ¾è§£é‡Šçš„å‘½ä»¤å¼æ­¥éª¤æè¿°è§£å†³æ–¹æ¡ˆæ¥è§£å†³é—®é¢˜ï¼Œ\
å¹¶ä¸”ä½ çŸ¥é“å¦‚ä½•é€‰æ‹©åœ¨æ—¶é—´å¤æ‚æ€§å’Œç©ºé—´å¤æ‚æ€§ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡çš„è§£å†³æ–¹æ¡ˆã€‚

å…·ä½“é—®é¢˜å¦‚ä¸‹:
{input}"""

other_template = """ä½ æ˜¯ä¸€ä¸ªaiå°åŠ©æ‰‹ï¼Œ\
å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚\

å…·ä½“é—®é¢˜å¦‚ä¸‹ï¼š
{input}"""
```

ï¼ˆ2ï¼‰ï¼Œå®šä¹‰æ¨¡ç‰ˆé“¾

```
# 3ï¼Œå®šä¹‰é“¾

physics_chain = ChatPromptTemplate.from_template(physics_template) | chat | StrOutputParser()

math_chain = ChatPromptTemplate.from_template(math_template) | chat | StrOutputParser()

history_chain = ChatPromptTemplate.from_template(history_template) | chat | StrOutputParser()

computer_chain = ChatPromptTemplate.from_template(computer_template) | chat | StrOutputParser()

other_chain = ChatPromptTemplate.from_template(other_template) | chat | StrOutputParser()
```

ï¼ˆ3ï¼‰ï¼Œå®šä¹‰åˆ†ç±»æ¨¡ç‰ˆå’Œåˆ†ç±»é“¾

```
# 4,å®šä¹‰åˆ†ç±»æ¨¡ç‰ˆå’Œåˆ†ç±»é“¾
classify_prompt_template = """
è¯·ä½ å¯¹ä»¥ä¸‹é—®é¢˜åˆ†ç±»åˆ†ç±»ä¸º"ç‰©ç†"ã€"æ•°å­¦"ã€"å†å²"ã€"è®¡ç®—æœº"ã€"å…¶ä»–"ï¼Œä¸­çš„ä¸€ä¸ªï¼Œä¸éœ€è¦è¿”å›å¤šä¸ªåˆ†ç±»ï¼Œè¿”å›ä¸€ä¸ªå³å¯ã€‚
å…·ä½“é—®é¢˜å¦‚ä¸‹ï¼š
{input}"""

classify_chain = ChatPromptTemplate.from_template(classify_prompt_template) | chat | StrOutputParser()
```

ï¼ˆ4ï¼‰ï¼Œä½¿ç”¨RunnableLambdaæ„å»ºè·¯ç”±é“¾(åˆ†ç±»é“¾)

```
# 5,ä½¿ç”¨RunnableLambdaæ„å»ºè·¯ç”±é“¾

def route_classification(text):
    # è·å–åˆ†ç±»ç»“æœ
    classification = classify_chain.invoke({"input": text})

    # æ ¹æ®åˆ†ç±»ç»“æœé€‰æ‹©å¯¹åº”çš„é“¾
    if "ç‰©ç†" in classification:
        return {"category": "ç‰©ç†", "chain": physics_chain}
    elif "æ•°å­¦" in classification:
        return {"category": "æ•°å­¦", "chain": math_chain}
    elif "å†å²" in classification:
        return {"category": "å†å²", "chain": history_chain}
    elif "è®¡ç®—æœº" in classification:
        return {"category": "è®¡ç®—æœº", "chain": computer_chain}
    else:
        return {"category": "å…¶ä»–", "chain": other_chain}

runnable_chain = RunnableLambda(route_classification)
```

ï¼ˆ5ï¼‰ï¼Œè·å–è¾“å‡º

```
# 6,è·å–è¾“å‡º

# è°ƒç”¨å¹¶è·å–åˆ†ç±»å’Œé“¾
text = "æ¨èä¸€ä¸‹æ•°æ®ç»“æ„å’Œç®—æ³•çš„å­¦ä¹ ä¹¦ç±"
result = runnable_chain.invoke(text)

category = result["category"]
chain = result["chain"]

# ä½¿ç”¨åˆ†ç±»å¯¹åº”çš„é“¾è¿›è¡Œå¤„ç†ï¼Œå¾—åˆ°å®é™…å›ç­”
response = chain.invoke({"input": text})

# æ‰“å°åˆ†ç±»å’Œå›ç­”
print(f"é—®é¢˜ç±»å‹: {category}")
print(f"å›ç­”: {response}")
```

**`route_classification`** è¿”å›çš„æ˜¯ä¸€ä¸ªåŒ…å« `category` å’Œ `chain` çš„å­—å…¸ã€‚`category` æ˜¯é—®é¢˜çš„åˆ†ç±»ï¼Œ`chain` æ˜¯å¯¹åº”çš„å¤„ç†é“¾ã€‚

ï¼ˆ6ï¼‰ï¼Œå®Œæ•´ç¤ºä¾‹

```
import os
from dotenv import load_dotenv, find_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

# 1,åŠ è½½æ¨¡å‹

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv(find_dotenv())

# åˆ›å»º ChatOpenAI å®ä¾‹å¹¶ä¼ å…¥ API å¯†é’¥
chat = ChatOpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com/v1",
    temperature=0.0,
    model='deepseek-chat'
)

# 2ï¼Œå®šä¹‰æ¨¡ç‰ˆ

physics_template = """ä½ æ˜¯ä¸€ä¸ªéå¸¸èªæ˜çš„ç‰©ç†ä¸“å®¶ã€‚ \
ä½ æ“…é•¿ç”¨ä¸€ç§ç®€æ´å¹¶ä¸”æ˜“äºç†è§£çš„æ–¹å¼å»å›ç­”é—®é¢˜ã€‚\
å½“ä½ ä¸çŸ¥é“é—®é¢˜çš„ç­”æ¡ˆæ—¶ï¼Œä½ æ‰¿è®¤\
ä½ ä¸çŸ¥é“.

å…·ä½“é—®é¢˜å¦‚ä¸‹:
{input}"""

math_template = """ä½ æ˜¯ä¸€ä¸ªéå¸¸ä¼˜ç§€çš„æ•°å­¦å®¶ã€‚ \
ä½ æ“…é•¿å›ç­”æ•°å­¦é—®é¢˜ã€‚ \
ä½ ä¹‹æ‰€ä»¥å¦‚æ­¤ä¼˜ç§€ï¼Œ \
æ˜¯å› ä¸ºä½ èƒ½å¤Ÿå°†æ£˜æ‰‹çš„é—®é¢˜åˆ†è§£ä¸ºç»„æˆéƒ¨åˆ†ï¼Œ\
å›ç­”ç»„æˆéƒ¨åˆ†ï¼Œç„¶åå°†å®ƒä»¬ç»„åˆåœ¨ä¸€èµ·ï¼Œå›ç­”æ›´å¹¿æ³›çš„é—®é¢˜ã€‚

å…·ä½“é—®é¢˜å¦‚ä¸‹ï¼š
{input}"""

history_template = """ä½ æ˜¯ä»¥ä¸ºéå¸¸ä¼˜ç§€çš„å†å²å­¦å®¶ã€‚ \
ä½ å¯¹ä¸€ç³»åˆ—å†å²æ—¶æœŸçš„äººç‰©ã€äº‹ä»¶å’ŒèƒŒæ™¯æœ‰ç€æå¥½çš„å­¦è¯†å’Œç†è§£\
ä½ æœ‰èƒ½åŠ›æ€è€ƒã€åæ€ã€è¾©è¯ã€è®¨è®ºå’Œè¯„ä¼°è¿‡å»ã€‚\
ä½ å°Šé‡å†å²è¯æ®ï¼Œå¹¶æœ‰èƒ½åŠ›åˆ©ç”¨å®ƒæ¥æ”¯æŒä½ çš„è§£é‡Šå’Œåˆ¤æ–­ã€‚

å…·ä½“é—®é¢˜å¦‚ä¸‹:
{input}"""

computer_template = """ ä½ æ˜¯ä¸€ä¸ªæˆåŠŸçš„è®¡ç®—æœºç§‘å­¦ä¸“å®¶ã€‚\
ä½ æœ‰åˆ›é€ åŠ›ã€åä½œç²¾ç¥ã€\
å‰ç»æ€§æ€ç»´ã€è‡ªä¿¡ã€è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€\
å¯¹ç†è®ºå’Œç®—æ³•çš„ç†è§£ä»¥åŠå‡ºè‰²çš„æ²Ÿé€šæŠ€å·§ã€‚\
ä½ éå¸¸æ“…é•¿å›ç­”ç¼–ç¨‹é—®é¢˜ã€‚\
ä½ ä¹‹æ‰€ä»¥å¦‚æ­¤ä¼˜ç§€ï¼Œæ˜¯å› ä¸ºä½ çŸ¥é“  \
å¦‚ä½•é€šè¿‡ä»¥æœºå™¨å¯ä»¥è½»æ¾è§£é‡Šçš„å‘½ä»¤å¼æ­¥éª¤æè¿°è§£å†³æ–¹æ¡ˆæ¥è§£å†³é—®é¢˜ï¼Œ\
å¹¶ä¸”ä½ çŸ¥é“å¦‚ä½•é€‰æ‹©åœ¨æ—¶é—´å¤æ‚æ€§å’Œç©ºé—´å¤æ‚æ€§ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡çš„è§£å†³æ–¹æ¡ˆã€‚

å…·ä½“é—®é¢˜å¦‚ä¸‹:
{input}"""

other_template = """ä½ æ˜¯ä¸€ä¸ªaiå°åŠ©æ‰‹ï¼Œ\
å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚\

å…·ä½“é—®é¢˜å¦‚ä¸‹ï¼š
{input}"""

# 3ï¼Œå®šä¹‰é“¾

physics_chain = ChatPromptTemplate.from_template(physics_template) | chat | StrOutputParser()

math_chain = ChatPromptTemplate.from_template(math_template) | chat | StrOutputParser()

history_chain = ChatPromptTemplate.from_template(history_template) | chat | StrOutputParser()

computer_chain = ChatPromptTemplate.from_template(computer_template) | chat | StrOutputParser()

other_chain = ChatPromptTemplate.from_template(other_template) | chat | StrOutputParser()

# 4,å®šä¹‰åˆ†ç±»æ¨¡ç‰ˆå’Œåˆ†ç±»é“¾
classify_prompt_template = """
è¯·ä½ å¯¹ä»¥ä¸‹é—®é¢˜åˆ†ç±»åˆ†ç±»ä¸º"ç‰©ç†"ã€"æ•°å­¦"ã€"å†å²"ã€"è®¡ç®—æœº"ã€"å…¶ä»–"ï¼Œä¸­çš„ä¸€ä¸ªï¼Œä¸éœ€è¦è¿”å›å¤šä¸ªåˆ†ç±»ï¼Œè¿”å›ä¸€ä¸ªå³å¯ã€‚
å…·ä½“é—®é¢˜å¦‚ä¸‹ï¼š
{input}"""

classify_chain = ChatPromptTemplate.from_template(classify_prompt_template) | chat | StrOutputParser()

# 5,ä½¿ç”¨RunnableLambdaæ„å»ºè·¯ç”±é“¾

def route_classification(text):
    # è·å–åˆ†ç±»ç»“æœ
    classification = classify_chain.invoke({"input": text})

    # æ ¹æ®åˆ†ç±»ç»“æœé€‰æ‹©å¯¹åº”çš„é“¾
    if "ç‰©ç†" in classification:
        return {"category": "ç‰©ç†", "chain": physics_chain}
    elif "æ•°å­¦" in classification:
        return {"category": "æ•°å­¦", "chain": math_chain}
    elif "å†å²" in classification:
        return {"category": "å†å²", "chain": history_chain}
    elif "è®¡ç®—æœº" in classification:
        return {"category": "è®¡ç®—æœº", "chain": computer_chain}
    else:
        return {"category": "å…¶ä»–", "chain": other_chain}

runnable_chain = RunnableLambda(route_classification)

# 6,è·å–è¾“å‡º

# è°ƒç”¨å¹¶è·å–åˆ†ç±»å’Œé“¾
text = "æ¨èä¸€ä¸‹æ•°æ®ç»“æ„å’Œç®—æ³•çš„å­¦ä¹ ä¹¦ç±"
result = runnable_chain.invoke(text)

category = result["category"]
chain = result["chain"]

# ä½¿ç”¨åˆ†ç±»å¯¹åº”çš„é“¾è¿›è¡Œå¤„ç†ï¼Œå¾—åˆ°å®é™…å›ç­”
response = chain.invoke({"input": text})

# æ‰“å°åˆ†ç±»å’Œå›ç­”
print(f"é—®é¢˜ç±»å‹: {category}")
print(f"å›ç­”: {response}")
```



## 4ï¼Œæ„å»ºæ–‡æ¡£æ£€ç´¢é—®ç­”ç³»ç»Ÿ

### (1),åŸºäºå‘é‡æ£€ç´¢

ï¼ˆ1ï¼‰**æ–‡æ¡£åŠ è½½å™¨ (`TextLoader`)**

- **ä½œç”¨**ï¼šå°†å­˜å‚¨åœ¨æœ¬åœ°çš„æ–‡æœ¬æ–‡ä»¶åŠ è½½åˆ°ç¨‹åºä¸­ï¼Œä½œä¸ºæ£€ç´¢çš„æ–‡æ¡£æºã€‚

```\
from langchain_community.document_loaders import TextLoader  # textæ–‡æ¡£åŠ è½½å™¨

# æ–‡æ¡£åŠ è½½å™¨ï¼ŒæŒ‡å®šæ–‡ä»¶è·¯å¾„åŠç¼–ç æ–¹å¼ï¼ŒåŠ è½½æ–‡æœ¬æ–‡ä»¶
loader = TextLoader(file_path="Clothing_Introduction.txt", encoding="utf-8")  # æŒ‡å®šUTF-8ç¼–ç 
```

ï¼ˆ2ï¼‰**åµŒå…¥æ¨¡å‹ (`HuggingFaceEmbeddings`)**

- **ä½œç”¨**ï¼šå°†åŠ è½½çš„æ–‡æ¡£è½¬æ¢ä¸ºåµŒå…¥å‘é‡ï¼Œä¾¿äºåœ¨é«˜ç»´ç©ºé—´ä¸­è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼Œä»¥å®ç°é«˜æ•ˆçš„æ–‡æ¡£æ£€ç´¢ã€‚

```
from langchain_huggingface import HuggingFaceEmbeddings

# ä½¿ç”¨ Hugging Face çš„åµŒå…¥æ¨¡å‹ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥å‘é‡
embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
```

ï¼ˆ3ï¼‰**å‘é‡ç´¢å¼• (`VectorstoreIndexCreator`)**

- **ä½œç”¨**ï¼šæ ¹æ®åµŒå…¥å‘é‡åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“ï¼Œç”¨äºé«˜æ•ˆçš„ç›¸ä¼¼åº¦æ£€ç´¢ã€‚

```
# åˆ›å»ºå‘é‡ç´¢å¼•ï¼Œä½¿ç”¨æ–‡æ¡£åŠ è½½å™¨åŠ è½½çš„æ–‡æ¡£ï¼Œè½¬æ¢ä¸ºå‘é‡å¹¶å­˜å‚¨
index = VectorstoreIndexCreator(
    vectorstore_cls=DocArrayInMemorySearch,  # å‘é‡å­˜å‚¨ç±»
    embedding=embedding  # åµŒå…¥æ¨¡å‹
).from_loaders([loader])  # ä½¿ç”¨åŠ è½½å™¨åŠ è½½æ–‡æ¡£
```

ï¼ˆ4ï¼‰**æ£€ç´¢å™¨ (`Retriever`)**

- **ä½œç”¨**ï¼šæ ¹æ®ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢å‡ºä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚

```
# å°†å‘é‡å­˜å‚¨è½¬æ¢ä¸ºæ£€ç´¢å™¨ï¼Œç”¨äºæ ¹æ®å‘é‡è¿›è¡Œæ–‡æ¡£æ£€ç´¢
retriever = index.vectorstore.as_retriever()
```

ï¼ˆ5ï¼‰ **å¤„ç†é“¾ (`RetrievalQA`)**

- **ä½œç”¨**ï¼šå°†èŠå¤©æ¨¡å‹ä¸æ£€ç´¢å™¨ç»“åˆï¼Œå½¢æˆä¸€ä¸ªé—®ç­”æµç¨‹ã€‚å®ƒå…ˆä»æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œç„¶åå°†è¿™äº›ç‰‡æ®µä¼ é€’ç»™èŠå¤©æ¨¡å‹ä»¥ç”Ÿæˆç­”æ¡ˆã€‚

```
# åˆ›å»ºæ£€ç´¢é—®ç­”é“¾ï¼Œç»“åˆèŠå¤©æ¨¡å‹å’Œæ£€ç´¢å™¨
qa_stuff = RetrievalQA.from_chain_type(
    llm=chat,  # ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹
    chain_type="stuff",  # æ£€ç´¢æ–¹å¼ï¼š"stuff" ç±»å‹çš„é“¾ï¼Œå…ˆæ£€ç´¢ç›¸å…³ä¿¡æ¯å†ç”Ÿæˆç­”æ¡ˆ
    retriever=retriever  # æ£€ç´¢å™¨ï¼Œç”¨äºä»æ–‡æ¡£ä¸­æ£€ç´¢ä¿¡æ¯
)

```

ï¼ˆ6ï¼‰å®Œæ•´ç¤ºä¾‹

```
import os
from dotenv import load_dotenv, find_dotenv
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA  # ç”¨äºå®ç°åŸºäºæ–‡æ¡£æ£€ç´¢çš„é—®ç­”é“¾ã€‚
from langchain_community.document_loaders import TextLoader  # æ–‡æ¡£åŠ è½½å™¨
from langchain_community.vectorstores import DocArrayInMemorySearch  # ç”¨äºåœ¨å†…å­˜ä¸­å­˜å‚¨æ–‡æ¡£çš„å‘é‡è¡¨ç¤ºã€‚
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.indexes import VectorstoreIndexCreator # ç”¨äºåˆ›å»ºå’Œç®¡ç†å‘é‡ç´¢å¼•ã€‚

# åŸºæœ¬è®¾ç½®ï¼šåŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(find_dotenv())

# åŠ è½½èŠå¤©æ¨¡å‹ï¼ŒæŒ‡å®š API å¯†é’¥ã€æ¨¡å‹ç±»å‹å’Œå…¶ä»–å‚æ•°
chat = ChatOpenAI(
    openai_api_key=os.getenv("DEEPSEEK_API_KEY"),  # ä»ç¯å¢ƒå˜é‡ä¸­è·å– API å¯†é’¥
    openai_api_base="https://api.deepseek.com/v1",  # ä½¿ç”¨çš„ API åŸºåœ°å€
    model="deepseek-reasoner",  # ä½¿ç”¨çš„æ¨¡å‹ç±»å‹
    temperature=0.0  # è®¾ç½®ä¸º 0.0ï¼Œè¡¨ç¤ºç”Ÿæˆçš„å›ç­”æ›´åŠ ç¡®å®šæ€§
)

# æ–‡æ¡£åŠ è½½å™¨ï¼ŒæŒ‡å®šæ–‡ä»¶è·¯å¾„åŠç¼–ç æ–¹å¼ï¼ŒåŠ è½½æ–‡æœ¬æ–‡ä»¶
loader = TextLoader(file_path="Clothing_Introduction.txt", encoding="utf-8")  # æŒ‡å®šUTF-8ç¼–ç 

# ä½¿ç”¨ Hugging Face çš„åµŒå…¥æ¨¡å‹ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥å‘é‡
embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')

# åˆ›å»ºå‘é‡ç´¢å¼•ï¼Œä½¿ç”¨æ–‡æ¡£åŠ è½½å™¨åŠ è½½çš„æ–‡æ¡£ï¼Œè½¬æ¢ä¸ºå‘é‡å¹¶å­˜å‚¨
index = VectorstoreIndexCreator(
    vectorstore_cls=DocArrayInMemorySearch,  # å‘é‡å­˜å‚¨ç±»
    embedding=embedding  # åµŒå…¥æ¨¡å‹
).from_loaders([loader])  # ä½¿ç”¨åŠ è½½å™¨åŠ è½½æ–‡æ¡£

# ç”¨æˆ·çš„é—®é¢˜
question = "è¿™ä»¶è¡£æœæœ‰ä»€ä¹ˆç‰¹ç‚¹"

# å°†å‘é‡å­˜å‚¨è½¬æ¢ä¸ºæ£€ç´¢å™¨ï¼Œç”¨äºæ ¹æ®å‘é‡è¿›è¡Œæ–‡æ¡£æ£€ç´¢
retriever = index.vectorstore.as_retriever()

# åˆ›å»ºæ£€ç´¢é—®ç­”é“¾ï¼Œç»“åˆèŠå¤©æ¨¡å‹å’Œæ£€ç´¢å™¨
qa_stuff = RetrievalQA.from_chain_type(
    llm=chat,  # ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹
    chain_type="stuff",  # "stuff" ç±»å‹çš„é“¾ï¼Œå…ˆæ£€ç´¢ç›¸å…³ä¿¡æ¯å†ç”Ÿæˆç­”æ¡ˆ
    retriever=retriever  # æ£€ç´¢å™¨ï¼Œç”¨äºä»æ–‡æ¡£ä¸­æ£€ç´¢ä¿¡æ¯
)

# æ‰§è¡Œé—®ç­”ä»»åŠ¡ï¼Œä¼ å…¥ç”¨æˆ·é—®é¢˜å¹¶è·å–æ¨¡å‹ç”Ÿæˆçš„å›ç­”
response = qa_stuff.invoke(question)

# è¾“å‡ºæ¨¡å‹çš„å›ç­”
print(f"å›å¤ï¼š\n{response['result']}")
```

### (2),åŸºäºRAG

ï¼ˆ1ï¼‰å¯¼å…¥æ–‡æ¡£

```
from langchain_community.document_loaders import TextLoader

try:
    loader = TextLoader(
        "Clothing_Introduction.txt",
        encoding="utf-8",
        autodetect_encoding=True  # æ·»åŠ è‡ªåŠ¨ç¼–ç æ£€æµ‹
    )
    docs = loader.load()
except Exception as e:
    raise RuntimeError(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
```

ï¼ˆ2ï¼‰åµŒå…¥æ¨¡å‹

```
from langchain_huggingface import HuggingFaceEmbeddings

embedding = HuggingFaceEmbeddings(
    model_name='sentence-transformers/all-MiniLM-L6-v2',
    model_kwargs={'device': 'cpu'},  # æ˜ç¡®æŒ‡å®šè¿è¡Œè®¾å¤‡
    encode_kwargs={'normalize_embeddings': True} # å½’ä¸€åŒ–å‘é‡
)
```

ï¼ˆ3ï¼‰å‘é‡ç´¢å¼•

```
from langchain_community.vectorstores import FAISS

# åˆ›å»ºå‘é‡æ•°æ®åº“
vectorstore = FAISS.from_documents(docs, embedding)
```

ä½œç”¨ï¼šå°†æ–‡æ¡£åˆ†å‰²ä¸ºå— â†’ ç”Ÿæˆå‘é‡ â†’ å»ºç«‹é«˜æ•ˆç´¢å¼•

ï¼ˆ4ï¼‰åˆ›å»ºæç¤ºæ¨¡ç‰ˆ

```
from langchain.prompts import ChatPromptTemplate

prompt_template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœè£…çŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚
å¦‚æœä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·å¦‚å®å‘ŠçŸ¥ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{input}
"""
prompt = ChatPromptTemplate.from_template(prompt_template)
```

ï¼ˆ5ï¼‰åˆ›å»ºå¤„ç†é“¾

```
# åˆ›å»ºæ£€ç´¢é—®ç­”é“¾
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

combine_chain = create_stuff_documents_chain(
    llm=llm,
    prompt=prompt
)

retrieval_chain = create_retrieval_chain(
    retriever=vectorstore.as_retriever(
        search_type="similarity",  # ä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢
        search_kwargs={"k": 3}     # è¿”å›å‰3ä¸ªç›¸å…³ç»“æœ
    ),
    combine_docs_chain=combine_chain
)
```

**`create_stuff_documents_chain`**: è¿™æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œæ¥å—ä¸€ä¸ª **LLM** å’Œä¸€ä¸ª **prompt** æ¥åˆ›å»ºæ–‡æ¡£å¤„ç†é“¾

**`create_retrieval_chain`**: è¿™ä¹Ÿæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œæ¥å— **retriever**ï¼ˆä¿¡æ¯æ£€ç´¢å™¨ï¼‰å’Œ **combine_docs_chain**ï¼ˆæ–‡æ¡£å¤„ç†é“¾ï¼‰ä½œä¸ºè¾“å…¥ï¼Œåˆ›å»ºä¸€ä¸ªæ•´ä½“çš„æ£€ç´¢å’Œæ–‡æ¡£å¤„ç†æµç¨‹ã€‚å…¶ä¸»è¦ä½œç”¨æ˜¯é€šè¿‡`retriever`æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶ä¼ é€’ç»™ `combine_docs_chain` æ¥å¤„ç†ç»“æœã€‚

`vectorstore` æ˜¯ä¸€ä¸ªå­˜å‚¨æ–‡æ¡£å‘é‡ï¼ˆé€šå¸¸ç”¨äºå‘é‡æœç´¢ï¼‰çš„å¯¹è±¡ï¼Œå®ƒèƒ½å¤Ÿå°†ç”¨æˆ·è¾“å…¥ä¸æ–‡æ¡£åº“ä¸­çš„å‘é‡è¿›è¡Œå¯¹æ¯”ï¼Œæ‰¾å‡ºæœ€ç›¸å…³çš„æ–‡æ¡£ã€‚

`as_retriever(...)` æ˜¯ä¸€ç§å°† `vectorstore` è½¬æ¢ä¸ºæ£€ç´¢å™¨çš„æ–¹å¼ã€‚å®ƒé€šè¿‡æŒ‡å®šæœç´¢ç±»å‹ `similarity` æ¥è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œå¹¶è®¾ç½® `search_kwargs={"k": 3}` æ¥é™åˆ¶æ£€ç´¢ç»“æœçš„æ•°é‡ï¼ˆå³æ£€ç´¢å‡ºä¸æŸ¥è¯¢æœ€ç›¸å…³çš„ **3** ä¸ªæ–‡æ¡£ï¼‰

é—®ï¼š`vectorstore` æœ¬èº«èƒ½å¤Ÿå®ç°æ–‡æ¡£çš„ **ç›¸ä¼¼æ€§æœç´¢**ï¼Œä¸ºä»€ä¹ˆè¿˜éœ€è¦`as_retriever()` è¿™ä¸ªè½¬æ¢æ­¥éª¤ï¼Ÿ

ç­”ï¼šè™½ç„¶ `vectorstore` å¯ä»¥è¿›è¡ŒåŸºç¡€çš„å‘é‡æ£€ç´¢ï¼Œä½† `as_retriever()` çš„ä¸»è¦ä½œç”¨æ˜¯æä¾›ä¸€ä¸ªæ›´çµæ´»ã€æ›´ç»Ÿä¸€çš„æ¥å£ï¼Œä¾¿äºä¸å…¶ä»–ç»„ä»¶é›†æˆï¼Œæ–¹ä¾¿å‚æ•°é…ç½®å’Œä¼˜åŒ–æ£€ç´¢è¿‡ç¨‹ï¼Œå¹¶ç®€åŒ–ä¸æ–‡æ¡£å¤„ç†é“¾ï¼ˆä¾‹å¦‚ `combine_docs_chain`ï¼‰çš„äº¤äº’ã€‚

ï¼ˆ6ï¼‰é—®ç­”å¤„ç†

```
def ask_question(question):
    try:
        response = retrieval_chain.invoke({"input": question})
        return response["answer"]
    except Exception as e:
        return f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        
if __name__ == "__main__":
    question = "è¿™ä»¶è¡£æœæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
    answer = ask_question(question)
    print(f"é—®é¢˜ï¼š{question}")
    print(f"å›å¤ï¼š{answer}")
```

è§£é‡Šä»£ç æµç¨‹ï¼š

1. ç”¨æˆ·æé—®ï¼ˆ `"è¿™ä»¶è¡£æœæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"`ï¼‰ã€‚
2. `retrieval_chain` ä½¿ç”¨è¿™ä¸ªé—®é¢˜åœ¨å‘é‡æ•°æ®åº“ä¸­è¿›è¡Œæ£€ç´¢ï¼Œå¾—åˆ°ä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚
3. æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹è¢«å¡«å……åˆ° `prompt_template` çš„ `{context}` éƒ¨åˆ†ã€‚
4. æœ€ç»ˆç”Ÿæˆçš„æç¤ºä¿¡æ¯ä¼šä¼ é€’ç»™èŠå¤©æ¨¡å‹ `llm` è¿›è¡Œå›ç­”ã€‚

ï¼ˆ7ï¼‰å®Œæ•´ç¤ºä¾‹

```
import os
from dotenv import load_dotenv, find_dotenv
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.prompts import ChatPromptTemplate

# åŸºæœ¬è®¾ç½®ï¼šåŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(find_dotenv())

# åˆå§‹åŒ–æ¨¡å‹æ—¶æ·»åŠ è¯¦ç»†å‚æ•°è¯´æ˜
llm = ChatOpenAI(
    openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
    openai_api_base="https://api.deepseek.com/v1",
    model="deepseek-chat",  # ç¡®è®¤ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åç§°
    temperature=0.0,
    max_tokens=512  # ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡é™åˆ¶ä¸º 512
)

# 1. æ–‡æ¡£åŠ è½½
try:
    loader = TextLoader(
        "Clothing_Introduction.txt",
        encoding="utf-8",
        autodetect_encoding=True  # æ·»åŠ è‡ªåŠ¨ç¼–ç æ£€æµ‹
    )
    docs = loader.load()
except Exception as e:
    raise RuntimeError(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")

# 2. åµŒå…¥æ¨¡å‹
embedding = HuggingFaceEmbeddings(
    model_name='sentence-transformers/all-MiniLM-L6-v2',
    model_kwargs={'device': 'cpu'},  # æ˜ç¡®æŒ‡å®šè¿è¡Œè®¾å¤‡
    encode_kwargs={'normalize_embeddings': True} # å½’ä¸€åŒ–å‘é‡
)

# 3. å‘é‡æ•°æ®åº“åˆ›å»º
vectorstore = FAISS.from_documents(docs, embedding)

# 4. åˆ›å»ºæç¤ºæ¨¡æ¿
prompt_template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœè£…çŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚
å¦‚æœä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·å¦‚å®å‘ŠçŸ¥ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{input}
"""
prompt = ChatPromptTemplate.from_template(prompt_template)

# 5. æ„å»ºå¤„ç†é“¾
combine_chain = create_stuff_documents_chain(
    llm=llm,
    prompt=prompt
)

retrieval_chain = create_retrieval_chain(
    retriever=vectorstore.as_retriever(
        search_type="similarity",  # ä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢
        search_kwargs={"k": 3}     # è¿”å›å‰3ä¸ªç›¸å…³ç»“æœ
    ),
    combine_docs_chain=combine_chain
)

# 6. é—®ç­”å‡½æ•°
def ask_question(question):
    try:
        response = retrieval_chain.invoke({"input": question})
        return response["answer"]
    except Exception as e:
        return f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    question = "è¿™ä»¶è¡£æœåˆé€‚åœ¨ä»€ä¹ˆåœºæ‰€ç©¿ï¼Ÿ"
    answer = ask_question(question)
    print(f"é—®é¢˜ï¼š{question}")
    print(f"å›å¤ï¼š{answer}")
```



## 5ï¼Œæ™ºèƒ½ä½“

### ï¼ˆ1ï¼‰ä½¿ç”¨LangGraphæ„å»ºä¸€ä¸ªå¯ä»¥ä½¿ç”¨æœç´¢å·¥å…·çš„èŠå¤©æœºå™¨äºº

ï¼ˆ1ï¼‰å¯¼å…¥åº“

```
from typing import Annotated, Typeddict # ç”¨äºç±»å‹å£°æ˜
import os  # æ“ä½œç³»ç»Ÿæ¥å£ï¼Œç”¨äºè®¿é—®ç¯å¢ƒå˜é‡
from dotenv import load_dotenv, find_dotenv  # ç”¨äºåŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
from langchain_openai import ChatOpenAI  # OpenAIèŠå¤©æ¨¡å‹æ¥å£
from langchain_community.tools.tavily_search import TavilySearchResults  # Tavilyå·¥å…·ï¼Œç”¨äºæœç´¢
from langgraph.graph import StateGraph  # LangGraphçš„å›¾å½¢ç»“æ„
from langgraph.graph.message import add_messages  # ç”¨äºå¤„ç†æ¶ˆæ¯çš„å‡½æ•°
from langgraph.prebuilt import ToolNode, tools_condition  # é¢„å®šä¹‰çš„å·¥å…·èŠ‚ç‚¹å’Œæ¡ä»¶åˆ†æ”¯
```

ï¼ˆ2ï¼‰ä¸ºèŠå¤©æœºå™¨äººå®šä¹‰ä¸€ä¸ª**ç»Ÿä¸€çš„æ•°æ®æ ¼å¼**ï¼Œå¹¶åˆ›å»ºStateGraphå®ä¾‹

```
class State(TypedDict):
    messages: Annotated[list, add_messages]
```

- é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ªæ•°æ®ç±»å‹`State`,å®ƒæ˜¯ä¸ªå­—å…¸ç±»å‹ï¼Œä½†å¿…é¡»åŒ…å«å­—æ®µ`messages`ï¼ˆç±»å‹ä¸ºlistï¼‰

```
# ç¤ºä¾‹
State = {
    "messages": [
        {"role": "user", "content": "ä½ å¥½ï¼"},
        {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿ"},
    ]
}
```

- `list` åç»­æ·»åŠ çš„å†…å®¹ï¼š

```
[{"role": "user", "content": "Hello!"},
  {"role": "assistant", "content": "Hi there!"}]
  
# è¿™æ˜¯ OpenAI Chat API çš„æ ‡å‡†æ¶ˆæ¯æ ¼å¼ï¼Œæ¯æ¡æ¶ˆæ¯ç”± `role` å’Œ `content` ä¸¤éƒ¨åˆ†ç»„æˆã€‚
```

- `Annotated`: ä¸ºlistæ·»åŠ é™„åŠ æ¶ˆæ¯ï¼Œå¹¶è¢«langgraphè¯†åˆ«å¹¶ä½¿ç”¨ã€‚

- `add_messages`:langgraphæ¡†æ¶åœ¨èŠå¤©è¿‡ç¨‹ä¸­ä¼šè¯†åˆ«è¿™ä¸ªé™„åŠ ä¿¡æ¯ï¼ŒæŠŠæ–°æ¶ˆæ¯ append åˆ°é‡Œé¢ï¼Œè€Œä¸æ˜¯é‡æ–°èµ‹å€¼æ•´ä¸ªåˆ—è¡¨ï¼ˆè¦†ç›–ï¼‰ã€‚

```
graph_builder = StateGraph(State)
```

`StateGraph` æ˜¯ LangGraph ä¸­çš„æ ¸å¿ƒç±»ï¼Œç”¨äºæ„å»ºå’Œç®¡ç†å¯¹è¯æµç¨‹å›¾

æ‰€æœ‰èŠ‚ç‚¹å¤„ç†çš„â€˜çŠ¶æ€ä¿¡æ¯â€™ï¼ˆä¹Ÿå°±æ˜¯ä¸Šä¸‹æ–‡ã€æ•°æ®ï¼‰éƒ½å¿…é¡»ç¬¦åˆ `State` ç±»å‹ï¼ˆæ•°æ®æ ¼å¼ï¼‰

ï¼ˆ3ï¼‰åˆ›å»ºå·¥å…·èŠ‚ç‚¹

è®¾ç½®å·¥å…·

```
tool = TavilySearchResults(max_results=2) # æœ€å¤šè¿”å›ä¸¤ä¸ªæœç´¢ç»“æœ
tools = [tool] # å°† tool æ”¾å…¥ä¸€ä¸ªå·¥å…·åˆ—è¡¨ tools ä¸­
```

å°†å·¥å…·ç»‘å®šåˆ° LLM

```
llm_with_tools = llm.bind_tools(tools)
```

`bind_tools()`: æ˜¯ä¸€ä¸ªæ–¹æ³•ï¼Œå®ƒ**æŠŠå·¥å…·çš„è°ƒç”¨èƒ½åŠ›â€œç»‘å®šâ€åˆ°è¿™ä¸ª LLM ä¸Š**ã€‚

`llm_with_tools`: å¾—åˆ°ä¸€ä¸ªæ–°çš„æ¨¡å‹å®ä¾‹ï¼Œå®ƒåœ¨æ¨ç†æ—¶ä¼šè‡ªåŠ¨è¯†åˆ«æ˜¯å¦åº”è¯¥è°ƒç”¨å·¥å…·ï¼Œå¹¶è¿”å›å·¥å…·è°ƒç”¨è¯·æ±‚ã€‚

- é€šè¿‡`bind_tools()`æ–¹æ³•å°†å·¥å…·æè¿°ä¿¡æ¯ï¼ˆåç§°ã€æè¿°ã€å‚æ•°æ ¼å¼ï¼‰æ³¨å…¥æ¨¡å‹çš„ç³»ç»Ÿæç¤ºä¸­

åˆ›å»ºå·¥å…·èŠ‚ç‚¹

```
tool_node = ToolNode(tools=[tool])
```

`Toolnode`ï¼šé¢„å®šä¹‰çš„å·¥å…·è°ƒç”¨å‡½æ•°

ï¼ˆ4ï¼‰åˆ›å»ºllmèŠ‚ç‚¹

å®šä¹‰`chatbot`å‡½æ•°

```
def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}
```

å®šä¹‰äº†ä¸€ä¸ª `chatbot` å‡½æ•°ï¼Œå®ƒæ¥å—ä¸€ä¸ª `State` ç±»å‹çš„å‚æ•° `state`ã€‚

è¯¥å‡½æ•°å°†å½“å‰å¯¹è¯çš„æ¶ˆæ¯ä¼ é€’ç»™æ¨¡å‹ï¼Œå¹¶è¿”å›ç”Ÿæˆçš„å›å¤ï¼ˆé€šè¿‡ `llm_with_tools.invoke` è°ƒç”¨æ¨¡å‹ï¼‰

ï¼ˆ5ï¼‰æ·»åŠ èŠ‚ç‚¹å’Œè¾¹å¹¶ç¼–è¯‘å›¾

```
# æ·»åŠ llmèŠ‚ç‚¹
graph_builder.add_node("chatbot", chatbot)
# æ·»åŠ å·¥å…·èŠ‚ç‚¹
graph_builder.add_node("tools", tool_node)
# æ·»åŠ æ¡ä»¶è¾¹
graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
# æ·»åŠ ä»å·¥å…·èŠ‚ç‚¹è¿”å›åˆ°llmèŠ‚ç‚¹çš„è¾¹
graph_builder.add_edge("tools", "chatbot")
# æ·»åŠ å…¥å£ç‚¹
graph_builder.set_entry_point("chatbot")

# ç¼–è¯‘å›¾
graph = graph_builder.compile()
```

- `dd_conditional_edges` æ–¹æ³•æ·»åŠ äº†æ¡ä»¶åˆ†æ”¯ï¼Œç”¨äºåœ¨ `chatbot` èŠ‚ç‚¹ä¹‹åæ ¹æ®æŸäº›æ¡ä»¶ï¼ˆå¦‚å·¥å…·æ˜¯å¦è¢«è°ƒç”¨ï¼‰è¿›è¡Œåˆ†æ”¯ã€‚
- `tools_condition` æ˜¯ä¸€ä¸ªé¢„å®šä¹‰çš„æ¡ä»¶å‡½æ•°ï¼Œç”¨äºå†³å®šæ˜¯å¦æ‰§è¡Œå·¥å…·èŠ‚ç‚¹æˆ–è¿”å›ENDã€‚
- `add_edge` è®¾ç½®äº†ä» `tools` èŠ‚ç‚¹åˆ° `chatbot` èŠ‚ç‚¹çš„è¾¹ï¼Œè¡¨ç¤ºå½“å·¥å…·èŠ‚ç‚¹æ‰§è¡Œåä¼šè¿”å›åˆ° `chatbot` èŠ‚ç‚¹ã€‚
- `set_entry_point("chatbot")` è®¾ç½®äº†å›¾çš„å…¥å£ç‚¹ä¸º `chatbot`ï¼Œå³ä» `chatbot` èŠ‚ç‚¹å¼€å§‹å¯¹è¯æµç¨‹ã€‚
- `compile()` æ–¹æ³•å°†æ„å»ºçš„å›¾å½¢ç¼–è¯‘æˆä¸€ä¸ªå¯ä»¥æ‰§è¡Œçš„å¯¹è¯æµç¨‹

ï¼ˆ6ï¼‰å¯åŠ¨å›¾å¹¶æµå¼è¾“å‡º

```
def stream_graph_updates(user_input: str):
    for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): # è¾“å…¥æ•°æ®ç¬¦åˆStateæ ¼å¼
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)
```

`graph.stream(...) `:æµå¼è¾“å‡ºå‡½æ•°ï¼š

LangGraph ä¼šå¯åŠ¨ä½ å®šä¹‰çš„æµç¨‹å›¾ï¼Œä»**èµ·å§‹èŠ‚ç‚¹**å¼€å§‹å¤„ç†ã€‚

- åœ¨å¤„ç†è¿‡ç¨‹ä¸­ï¼Œå›¾ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½ä¼šç”¨è¿™ä¸ªè¾“å…¥æ•°æ®ï¼ˆ`{"messages": [{"role": "user", "content": user_input}]}`ï¼‰ä½œä¸ºåˆå§‹çŠ¶æ€ï¼Œé€æ­¥å‘ä¸‹ä¼ é€’ã€‚
- æ¯”å¦‚ï¼Œä½ çš„ç¬¬ä¸€ä¸ªèŠ‚ç‚¹å¯èƒ½æ˜¯ `chatbot`ï¼Œå®ƒä¼šæ”¶åˆ°è¿™äº› `messages`ï¼Œç„¶åäº§ç”Ÿè‡ªå·±çš„è¾“å‡ºï¼ˆå¯èƒ½æ˜¯æ¨¡å‹çš„å›å¤ï¼‰ã€‚

æ¯å½“ä¸€ä¸ªèŠ‚ç‚¹æ‰§è¡Œå®Œæ¯•ï¼Œå®ƒå°±ä¼šäº§å‡ºä¸€ä¸ªâ€œäº‹ä»¶ï¼ˆeventï¼‰â€ã€‚è¿™ä¸ª `event` æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œkey æ˜¯èŠ‚ç‚¹çš„åå­—ï¼Œvalue æ˜¯è¿™ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºçŠ¶æ€ï¼ˆç¬¦åˆ `State` ç±»å‹ï¼‰ã€‚

æ¯”å¦‚ï¼š

```
{
  "chatbot": {
    "messages": [{"role": "assistant", "content": "ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ ï¼Ÿ"}]
  }
}
```

æ¯æ¬¡è°ƒç”¨ `graph.stream()`ï¼Œéƒ½ä¼šæœ‰ä¸€ä¸ª**ä¸­é—´ç»“æœï¼ˆeventï¼‰**æµå‡ºæ¥ã€‚

`event.values()`:è·å–æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºçŠ¶æ€

`value["messages"][-1].content`:è¾“å‡ºæœ€åä¸€ä¸ªä¹Ÿå°±æ˜¯æœ€æ–°çš„æ¶ˆæ¯å†…å®¹

ï¼ˆ7ï¼‰ä¸»å¾ªç¯

```
while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break
        stream_graph_updates(user_input)
    except:
        # fallback if input() is not available
        user_input = "What do you know about LangGraph?"
        print("User: " + user_input)
        stream_graph_updates(user_input)
        break
```

- ä¸»å¾ªç¯æ¥æ”¶ç”¨æˆ·è¾“å…¥å¹¶è°ƒç”¨ `stream_graph_updates` æ¥å¤„ç†è¾“å…¥å¹¶æ˜¾ç¤ºåŠ©æ‰‹çš„å›åº”ã€‚
- å¦‚æœç”¨æˆ·è¾“å…¥ `quit`ã€`exit` æˆ– `q`ï¼Œåˆ™é€€å‡ºå¾ªç¯å¹¶ç»“æŸå¯¹è¯ã€‚
- å¦‚æœ `input()` ä¸å¯ç”¨ï¼ˆå¯èƒ½æ˜¯åœ¨æŸäº›ç¯å¢ƒä¸­ï¼‰ï¼Œåˆ™ä¼šä½¿ç”¨é»˜è®¤è¾“å…¥ `"What do you know about LangGraph?"` è¿›è¡Œæµ‹è¯•ã€‚

ï¼ˆ8ï¼‰å®Œæ•´ç¤ºä¾‹

```
import os  # æ“ä½œç³»ç»Ÿæ¥å£ï¼Œç”¨äºè®¿é—®ç¯å¢ƒå˜é‡
from dotenv import load_dotenv, find_dotenv  # ç”¨äºåŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
from langchain_openai import ChatOpenAI  # OpenAIèŠå¤©æ¨¡å‹æ¥å£

from typing import Annotated, TypedDict
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_community.tools.tavily_search import TavilySearchResults

# 1,è®¾ç½®llmæ¨¡å‹
load_dotenv(find_dotenv())  # åŠ è½½ç¯å¢ƒå˜é‡

llm = ChatOpenAI(
    openai_api_key=os.getenv("OPENAI_API_KEY"),  # APIå¯†é’¥
    model="gpt-4o",  # ä½¿ç”¨çš„æ¨¡å‹
    temperature=0.0,  # ç”Ÿæˆçš„ç¡®å®šæ€§ï¼ˆ0æœ€ç¡®å®šï¼‰
    openai_api_base='http://apikey888.top/v1'
)

# 2ï¼Œåˆ›å»ºStateGraphå®ä¾‹
class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

# 3ï¼Œåˆ›å»ºå·¥å…·èŠ‚ç‚¹
tool = TavilySearchResults(max_results=2) # æœ€å¤šè¿”å›ä¸¤ä¸ªæœç´¢ç»“æœ
tools = [tool] # å°† tool æ”¾å…¥ä¸€ä¸ªå·¥å…·åˆ—è¡¨ tools ä¸­
llm_with_tools = llm.bind_tools(tools)
tool_node = ToolNode(tools=[tool])

# 4ï¼Œåˆ›å»ºllmèŠ‚ç‚¹
def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

# 5ï¼Œæ„é€ å¹¶ç¼–è¯‘å›¾
# æ·»åŠ llmèŠ‚ç‚¹
graph_builder.add_node("chatbot", chatbot)
# æ·»åŠ å·¥å…·èŠ‚ç‚¹
graph_builder.add_node("tools", tool_node)
# æ·»åŠ æ¡ä»¶è¾¹
graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
# æ·»åŠ ä»å·¥å…·èŠ‚ç‚¹è¿”å›åˆ°llmèŠ‚ç‚¹çš„è¾¹
graph_builder.add_edge("tools", "chatbot")
# æ·»åŠ å…¥å£ç‚¹
graph_builder.set_entry_point("chatbot")

# ç¼–è¯‘å›¾
graph = graph_builder.compile()

# 6ï¼Œæµå¼è¾“å‡ºè¿è¡Œå›¾
def stream_graph_updates(user_input: str):
    for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): # è¾“å…¥æ•°æ®ç¬¦åˆStateæ ¼å¼
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)

# 7ï¼Œä¸»å¾ªç¯
while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break
        stream_graph_updates(user_input)
    except:
        print("æ²¡æœ‰è¾“å…¥")
        break
```



### ï¼ˆ2ï¼‰ä¸ºèŠå¤©æœºå™¨äººæ·»åŠ å†…å­˜

ï¼ˆ1ï¼‰å¯¼å…¥æ¨¡å—

```
from langgraph.checkpoint.memory import MemorySaver
```

æä¾›äº†ä¸€ä¸ªæ£€æŸ¥ç‚¹ç®¡ç†ç»„ä»¶ï¼Œå…è®¸ä½ ä¿å­˜å›¾æ‰§è¡Œè¿‡ç¨‹ä¸­çš„**çŠ¶æ€ä¿¡æ¯**ï¼Œå¹¶åœ¨å¿…è¦æ—¶è¿›è¡Œ**æ¢å¤**

ï¼ˆ2ï¼‰åˆ›å»º`MemorySaver` å®ä¾‹

```
memory = MemorySaver()

graph = graph_builder.compile(checkpointer=memory)  # æ·»åŠ è®°å¿†åŠŸèƒ½

config = {"configurable": {"thread_id": "1"}} # å®šä¹‰é…ç½®é¡¹
```

`checkpointer=memory` ä¼ å…¥äº† `MemorySaver` å®ä¾‹ï¼Œæ„å‘³ç€åœ¨å›¾æ‰§è¡Œæ—¶ï¼ŒLangGraph ä¼šæ ¹æ® `MemorySaver` çš„é€»è¾‘å°†æ¯ä¸ªèŠ‚ç‚¹çš„çŠ¶æ€ä¿å­˜åˆ°å†…å­˜ä¸­ã€‚

ï¼ˆ3ï¼‰è®¾ç½®é…ç½®å¹¶ä¼ å…¥`graph.stream()`

```
config = {"configurable": {"thread_id": "1"}} # å®šä¹‰é…ç½®é¡¹

def stream_graph_updates(user_input: str):
    for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}, config): # ä¼ å…¥é…ç½®
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)

```

å®šä¹‰ä¸€ä¸ª `config` é…ç½®å­—å…¸ï¼Œç”¨äºå­˜å‚¨ä¸æ‰§è¡Œç›¸å…³çš„**å¯é…ç½®å‚æ•°**

`thread_id` è¿™é‡Œå°±è¡¨ç¤ºå¯¹è¯çš„â€œçº¿ç¨‹ IDâ€ã€‚

- å¦‚æœ thread_id ç›¸åŒï¼Œæ¯æ¬¡æ–°çš„ user_input éƒ½ä¼šç”¨åŒä¸€ä¸ªçŠ¶æ€ç»§ç»­æ¨ç†ï¼ˆæœ‰è®°å¿†ï¼‰ã€‚
- å¦‚æœ thread_id ä¸åŒï¼ŒLangGraph å°±ä¼šè®¤ä¸ºæ˜¯æ–°çš„å¯¹è¯ã€‚



### ï¼ˆ3ï¼‰è¡¥å……ï¼šæµå¼æ‰§è¡Œå’Œéæµå¼æ‰§è¡Œ

ï¼ˆ1ï¼‰æµå¼æ‰§è¡Œ

**æµå¼æ‰§è¡Œ**æŒ‡çš„æ˜¯æ•°æ®æˆ–è€…äº‹ä»¶çš„å¤„ç†æ˜¯é€æ­¥è¿›è¡Œçš„ï¼Œå¹¶ä¸”å®æ—¶ç”Ÿæˆè¾“å‡ºã€‚

æ¯å½“æŸä¸€éƒ¨åˆ†ä»»åŠ¡å®Œæˆæ—¶ï¼Œç»“æœå°±ä¼šç«‹åˆ»è¿”å›ï¼Œç”¨æˆ·å¯ä»¥ç«‹å³çœ‹åˆ°è¾“å‡ºã€‚

ï¼ˆ2ï¼‰éæµå¼æ‰§è¡Œ

**éæµå¼æ‰§è¡Œ**æŒ‡çš„æ˜¯æ•´ä¸ªä»»åŠ¡çš„æ‰§è¡Œè¿‡ç¨‹ä¼šä¸€æ¬¡æ€§å®Œæˆï¼Œç›´åˆ°æœ€ç»ˆç»“æœæ‰èƒ½è¿”å›

ç”¨æˆ·éœ€è¦ç­‰å¾…æ‰€æœ‰çš„ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œæ‰èƒ½è·å¾—æœ€ç»ˆç»“æœ

ï¼ˆ3ï¼‰å®Œæ•´ç¤ºä¾‹

```
import os  # æ“ä½œç³»ç»Ÿæ¥å£ï¼Œç”¨äºè®¿é—®ç¯å¢ƒå˜é‡
from dotenv import load_dotenv, find_dotenv  # ç”¨äºåŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
from langchain_openai import ChatOpenAI  # OpenAIèŠå¤©æ¨¡å‹æ¥å£

from typing import Annotated, TypedDict
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_community.tools.tavily_search import TavilySearchResults

from langgraph.checkpoint.memory import MemorySaver

# 1,è®¾ç½®llmæ¨¡å‹
load_dotenv(find_dotenv())  # åŠ è½½ç¯å¢ƒå˜é‡

llm = ChatOpenAI(
    openai_api_key=os.getenv("OPENAI_API_KEY"),  # APIå¯†é’¥
    model="gpt-4o",  # ä½¿ç”¨çš„æ¨¡å‹
    temperature=0.0,  # ç”Ÿæˆçš„ç¡®å®šæ€§ï¼ˆ0æœ€ç¡®å®šï¼‰
    openai_api_base='http://apikey888.top/v1'
)

# 2ï¼Œåˆ›å»ºStateGraphå®ä¾‹
class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

# 3ï¼Œåˆ›å»ºå·¥å…·èŠ‚ç‚¹
tool = TavilySearchResults(max_results=2) # æœ€å¤šè¿”å›ä¸¤ä¸ªæœç´¢ç»“æœ
tools = [tool] # å°† tool æ”¾å…¥ä¸€ä¸ªå·¥å…·åˆ—è¡¨ tools ä¸­
llm_with_tools = llm.bind_tools(tools)
tool_node = ToolNode(tools=[tool])

# 4ï¼Œåˆ›å»ºllmèŠ‚ç‚¹
def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

# 5ï¼Œæ„é€ å¹¶ç¼–è¯‘å›¾
# æ·»åŠ llmèŠ‚ç‚¹
graph_builder.add_node("chatbot", chatbot)
# æ·»åŠ å·¥å…·èŠ‚ç‚¹
graph_builder.add_node("tools", tool_node)
# æ·»åŠ æ¡ä»¶è¾¹
graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
# æ·»åŠ ä»å·¥å…·èŠ‚ç‚¹è¿”å›åˆ°llmèŠ‚ç‚¹çš„è¾¹
graph_builder.add_edge("tools", "chatbot")
# æ·»åŠ å…¥å£ç‚¹
graph_builder.set_entry_point("chatbot")

# ç¼–è¯‘å›¾
memory = MemorySaver()
graph = graph_builder.compile(checkpointer=memory) # æ·»åŠ è®°å¿†åŠŸèƒ½

config = {"configurable": {"thread_id": "1"}}

def stream_graph_updates(user_input: str):
    for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}, config): # è¾“å…¥æ•°æ®ç¬¦åˆStateæ ¼å¼
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)

# # éæµå¼çš„å®ç°
# def graph_updates(user_input: str):
#     result = graph.invoke({"messages": [{"role": "user", "content": user_input}]}, config) # è¾“å…¥æ•°æ®ç¬¦åˆStateæ ¼å¼
#     print("Assistant:", result["messages"][-1].content)

while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break
        stream_graph_updates(user_input)
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯{e}")
        break
```



### ï¼ˆ4ï¼‰å®Œæ•´å®ç°

```
import os  # æ“ä½œç³»ç»Ÿæ¥å£ï¼Œç”¨äºè®¿é—®ç¯å¢ƒå˜é‡
import json
from dotenv import load_dotenv, find_dotenv  # ç”¨äºåŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
from langchain_openai import ChatOpenAI  # OpenAIèŠå¤©æ¨¡å‹æ¥å£

from typing import Annotated, TypedDict
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import ToolMessage

from langgraph.checkpoint.memory import MemorySaver

# 1,è®¾ç½®llmæ¨¡å‹
load_dotenv(find_dotenv())  # åŠ è½½ç¯å¢ƒå˜é‡

llm = ChatOpenAI(
    openai_api_key=os.getenv("OPENAI_API_KEY"),  # APIå¯†é’¥
    model="gpt-4o",  # ä½¿ç”¨çš„æ¨¡å‹
    temperature=0.0,  # ç”Ÿæˆçš„ç¡®å®šæ€§ï¼ˆ0æœ€ç¡®å®šï¼‰
    openai_api_base='http://apikey888.top/v1'
)

# 2ï¼Œåˆ›å»ºStateGraphå®ä¾‹
class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

# 3ï¼Œåˆ›å»ºå·¥å…·èŠ‚ç‚¹
tool = TavilySearchResults(max_results=2) # æœ€å¤šè¿”å›ä¸¤ä¸ªæœç´¢ç»“æœ
tools = [tool] # å°† tool æ”¾å…¥ä¸€ä¸ªå·¥å…·åˆ—è¡¨ tools ä¸­
llm_with_tools = llm.bind_tools(tools)

class ToolNode:  # æ„å»ºå·¥å…·è°ƒç”¨å‡½æ•°

    def __init__(self, tools: list) -> None:
        self.tools_by_name = {tool.name: tool for tool in tools}

    def __call__(self, inputs: dict):
        messages = inputs.get("messages", [])
        if messages:
            message = messages[-1]
        else:
            raise ValueError("No message found in input")
        outputs = []
        for tool_call in message.tool_calls:
            tool_result = self.tools_by_name[tool_call["name"]].invoke(
                tool_call["args"]
            )
            outputs.append(
                ToolMessage(
                    content=json.dumps(tool_result),
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            )
        return {"messages": outputs}

tool_node = ToolNode(tools=[tool])

# 4ï¼Œåˆ›å»ºllmèŠ‚ç‚¹
def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

# 5, åˆ›å»ºå†³ç­–è¾¹åˆ¤æ–­å‡½æ•°
def route_tools(state: State):
    messages = state.get("messages", [])
    if messages:
        message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")

    if hasattr(message, "tool_calls") and len(message.tool_calls) > 0:
        return "tools"

    return END

# 6ï¼Œæ„é€ å¹¶ç¼–è¯‘å›¾
graph_builder.add_node("chatbot", chatbot) # æ·»åŠ llmèŠ‚ç‚¹
graph_builder.add_node("tools", tool_node) # æ·»åŠ å·¥å…·èŠ‚ç‚¹
graph_builder.add_conditional_edges(       # æ·»åŠ æ¡ä»¶è¾¹
    "chatbot",
    route_tools,
    {"tools": "tools", END: END},
)
graph_builder.add_edge("tools", "chatbot") # æ·»åŠ ä»å·¥å…·èŠ‚ç‚¹è¿”å›åˆ°llmèŠ‚ç‚¹çš„è¾¹
graph_builder.add_edge(START, "chatbot")  # æ·»åŠ å…¥å£ç‚¹

memory = MemorySaver()   # æ·»åŠ è®°å¿†åŠŸèƒ½
graph = graph_builder.compile(checkpointer=memory) # ç¼–è¯‘å›¾

# 7ï¼Œè®¾ç½®æµå¼è¾“å…¥
config = {"configurable": {"thread_id": "1"}} # é…ç½®é¡¹

def stream_graph_updates(user_input: str):
    for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}, config): # è¾“å…¥æ•°æ®ç¬¦åˆStateæ ¼å¼
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)

# 8ï¼Œä¸»å¾ªç¯
while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break
        stream_graph_updates(user_input)
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯{e}")
        break
```



```
class ToolNode:  # æ„å»ºå·¥å…·è°ƒç”¨å‡½æ•°
	
	# ç±»çš„åˆå§‹åŒ–æ–¹æ³•ã€‚å®ƒæ¥æ”¶ä¸€ä¸ª tools åˆ—è¡¨ï¼ˆåŒ…å«å¤šä¸ªå·¥å…·å¯¹è±¡ï¼‰
    def __init__(self, tools: list) -> None:
        self.tools_by_name = {tool.name: tool for tool in tools}

	# è¿™æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„æ–¹æ³•ï¼Œä½¿å¾— ToolNode ç±»çš„å®ä¾‹å¯ä»¥åƒå‡½æ•°ä¸€æ ·è¢«è°ƒç”¨ã€‚
    def __call__(self, inputs: dict):
        messages = inputs.get("messages", [])
        if messages:
            message = messages[-1]
        else:
            raise ValueError("No message found in input")
        outputs = []
        # éå† message ä¸­çš„æ¯ä¸ªå·¥å…·è°ƒç”¨ã€‚
        for tool_call in message.tool_calls:
            tool_result = self.tools_by_name[tool_call["name"]].invoke(
                tool_call["args"]
            )
            outputs.append(
                ToolMessage(
                    content=json.dumps(tool_result), # å·¥å…·ç»“æœçš„ JSON å­—ç¬¦ä¸²ã€‚
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            )
        return {"messages": outputs}
```

`messages = inputs.get("messages", [])`
è·å–æ¶ˆæ¯ï¼šä»è¾“å…¥çš„ inputs å­—å…¸ä¸­è·å– "messages" é”®å¯¹åº”çš„å€¼ã€‚å¦‚æœ messages å­˜åœ¨ï¼Œå®ƒå°†æ˜¯ä¸€ä¸ªåŒ…å«æ¶ˆæ¯çš„åˆ—è¡¨ï¼›å¦‚æœ messages ä¸å­˜åœ¨ï¼Œåˆ™é»˜è®¤ä¸ºç©ºåˆ—è¡¨ã€‚

å¯¹äºæ¯ä¸ª `tool_call`ï¼ˆå·¥å…·è°ƒç”¨ï¼‰ï¼Œé€šè¿‡å·¥å…·åç§°æŸ¥æ‰¾å¯¹åº”çš„å·¥å…·ï¼ˆ`self.tools_by_name[tool_call["name"]]`ï¼‰ã€‚ç„¶åè°ƒç”¨è¯¥å·¥å…·çš„ `invoke` æ–¹æ³•ï¼Œå¹¶ä¼ å…¥ `tool_call["args"]` ä½œä¸ºå‚æ•°



```5, åˆ›å»ºå†³ç­–è¾¹åˆ¤æ–­å‡½æ•°
# 5, åˆ›å»ºå†³ç­–è¾¹åˆ¤æ–­å‡½æ•°
def route_tools(state: State):
    messages = state.get("messages", [])
    if messages:
        message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")

    if hasattr(message, "tool_calls") and len(message.tool_calls) > 0:
        return "tools"
        
    return END
```

`hasattr` æ˜¯ Python å†…ç½®çš„å‡½æ•°ï¼Œç”¨æ¥æ£€æŸ¥ä¸€ä¸ªå¯¹è±¡æ˜¯å¦å…·æœ‰æŸä¸ªå±æ€§ã€‚

è¿™é‡Œæ£€æŸ¥ `message` å¯¹è±¡æ˜¯å¦æœ‰åä¸º `tool_calls` çš„å±æ€§ã€‚`tool_calls` é€šå¸¸æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«å½“å‰æ¶ˆæ¯ä¸­æ‰€éœ€è°ƒç”¨çš„å·¥å…·çš„ä¿¡æ¯ã€‚

å¦‚æœ `message` å¯¹è±¡æœ‰ `tool_calls` å±æ€§ï¼Œ`hasattr` è¿”å› `True`ï¼Œå¦åˆ™è¿”å› `False`

å¦‚æœ `message` åŒ…å« `tool_calls` å±æ€§ï¼Œå¹¶ä¸”è¯¥å±æ€§çš„é•¿åº¦å¤§äºé›¶ï¼ˆå³æœ‰å·¥å…·è°ƒç”¨ï¼‰ï¼Œåˆ™è¿”å›å­—ç¬¦ä¸² `"tools"`ï¼Œè¡¨ç¤ºæ¶ˆæ¯ä¸­å­˜åœ¨å·¥å…·è°ƒç”¨ã€‚



## 6ï¼ŒMCP

æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼Œ**æ ‡å‡†åŒ–è®©æ¨¡å‹ï¼ˆæ™ºèƒ½ä½“ï¼‰å’Œå¤–éƒ¨æ•°æ®ï¼ˆå·¥å…·/èµ„æºï¼‰çš„äº¤äº’**ã€‚

MCPåè®®æä¾›äº†ä¸€ç§**æ ‡å‡†åŒ–çš„æ¥å£**ï¼Œä½¿å¾—**ä¸åŒå¼€å‘è€…çš„æ™ºèƒ½ä½“**å’Œ**ä¸åŒçš„å·¥å…·**ä¹‹é—´å¯ä»¥ç›´æ¥äº¤äº’ï¼Œè€Œä¸éœ€è¦é‡æ–°å®ç°æ¥å£



 
